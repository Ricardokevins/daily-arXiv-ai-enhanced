<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [GEM: Empowering LLM for both Embedding Generation and Language Understanding](https://arxiv.org/abs/2506.04344)
*Caojin Zhang,Qiang Zhang,Ke Li,Sai Vidyaranya Nuthalapati,Benyu Zhang,Jason Liu,Serena Li,Lizhu Zhang,Xiangjun Fan*

Main category: cs.CL

TL;DR: 本文提出了一种名为GEM的自监督方法，使大型解码器LLM能够在保持其原始文本生成和推理能力的同时，生成高质量的文本嵌入。


<details>
  <summary>Details</summary>
Motivation: 现有的LLMs在生成任务中表现出色，但在需要文本嵌入的应用（如RAG）中，仍依赖独立的嵌入模型，这会使系统复杂化，并可能导致理解上的差异。

Method: 提出了一种名为“生成式嵌入大型语言模型（GEM）”的自监督方法。该方法通过在文本主体中插入新的特殊token并操纵注意力掩码来生成文本的摘要嵌入。可轻松集成到现有LLM的后训练或微调阶段。

Result: 该方法显著提高了LLMs在MTEB（文本嵌入基准）上的表现，同时对MMLU（NLP基准）的影响最小。

Conclusion: 所提出的方法能够赋予LLMs最先进的文本嵌入能力，同时保持其原始的NLP性能。

Abstract: Large decoder-only language models (LLMs) have achieved remarkable success in
generation and reasoning tasks, where they generate text responses given
instructions. However, many applications, e.g., retrieval augmented generation
(RAG), still rely on separate embedding models to generate text embeddings,
which can complicate the system and introduce discrepancies in understanding of
the query between the embedding model and LLMs. To address this limitation, we
propose a simple self-supervised approach, Generative Embedding large language
Model (GEM), that enables any large decoder-only LLM to generate high-quality
text embeddings while maintaining its original text generation and reasoning
capabilities. Our method inserts new special token(s) into a text body, and
generates summarization embedding of the text by manipulating the attention
mask. This method could be easily integrated into post-training or fine tuning
stages of any existing LLMs. We demonstrate the effectiveness of our approach
by applying it to two popular LLM families, ranging from 1B to 8B parameters,
and evaluating the transformed models on both text embedding benchmarks (MTEB)
and NLP benchmarks (MMLU). The results show that our proposed method
significantly improves the original LLMs on MTEB while having a minimal impact
on MMLU. Our strong results indicate that our approach can empower LLMs with
state-of-the-art text embedding capabilities while maintaining their original
NLP performance

</details>
