<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 38]
- [cs.CV](#cs.CV) [Total: 3]
- [cs.SD](#cs.SD) [Total: 1]
- [cs.LG](#cs.LG) [Total: 8]
- [cs.AI](#cs.AI) [Total: 5]
- [eess.AS](#eess.AS) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Towards Probabilistic Question Answering Over Tabular Data](https://arxiv.org/abs/2506.20747)
*Chen Shen,Sajjadur Rahman,Estevam Hruschka*

Main category: cs.CL

TL;DR: 本文介绍了一个用于表格数据概率问答的新基准LUCARIO和一个框架，该框架通过从表格中推断贝叶斯网络并将自然语言查询转换为概率查询，然后使用大型语言模型生成最终答案，显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 当前的表格数据问答方法（如NL2SQL系统）在事实性问题上表现良好，但无法处理需要不确定性推理的概率性问题。

Method: 引入了LUCARIO基准和概率问答框架。该方法从表格中推断贝叶斯网络，将自然语言查询转换为概率查询，并使用大型语言模型生成最终答案。

Result: 实证结果表明，与基线方法相比，该方法取得了显著改进。

Conclusion: 突出了混合符号-神经推理的优势。

Abstract: Current approaches for question answering (QA) over tabular data, such as
NL2SQL systems, perform well for factual questions where answers are directly
retrieved from tables. However, they fall short on probabilistic questions
requiring reasoning under uncertainty. In this paper, we introduce a new
benchmark LUCARIO and a framework for probabilistic QA over large tabular data.
Our method induces Bayesian Networks from tables, translates natural language
queries into probabilistic queries, and uses large language models (LLMs) to
generate final answers. Empirical results demonstrate significant improvements
over baselines, highlighting the benefits of hybrid symbolic-neural reasoning.

</details>


### [2] [Multi-lingual Functional Evaluation for Large Language Models](https://arxiv.org/abs/2506.20793)
*Victor Ojewale,Inioluwa Deborah Raji,Suresh Venkatasubramanian*

Main category: cs.CL

TL;DR: 现有静态多语言基准未能充分评估大型语言模型的实际跨语言性能和鲁棒性。研究者创建了新的多语言功能性基准（CL-GSM Symbolic和CL-IFEval），通过翻译现有英文模板到其他五种语言。结果显示，一些静态基准与功能性表现存在显著差异，且模型在不同语言间的鲁棒性差异很大（如阿拉伯语和英语表现更稳定）。


<details>
  <summary>Details</summary>
Motivation: 现有静态数据基准（如Belebele、M-MMLU和M-GSM）未能充分评估大型语言模型在多语言环境下的实际性能和鲁棒性。

Method: 通过将现有的英文功能性基准模板翻译成法语、西班牙语、印地语、阿拉伯语和约鲁巴语五种额外语言，创建了多语言功能性基准——跨语言小学数学符号（CL-GSM Symbolic）和跨语言指令遵循评估（CL-IFEval）。

Result: 结果显示，一些静态多语言基准比其他基准更能捕捉功能性性能（例如，M-GSM与CL-GSM Symbolic在英语、法语和西班牙语中的性能分别下降24%、17%和18%）；Belebele与CL-IFEval之间在跨语言中存在15-24%的性能下降，而M-MMLU与CL-IFEval之间仅有0.5%至3%的性能下降。模型在不同语言间的鲁棒性差异显著，某些语言（如阿拉伯语、英语）在评估迭代中表现最稳定。

Conclusion: 静态多语言基准评估可能无法充分反映大型语言模型的实际功能性表现和跨语言鲁棒性，需要更有效的功能性基准来全面评估模型。模型在不同语言上的鲁棒性表现不一。

Abstract: Multi-lingual competence in large language models is often evaluated via
static data benchmarks such as Belebele, M-MMLU and M-GSM. However, these
evaluations often fail to provide an adequate understanding of the practical
performance and robustness of models across multi-lingual settings. In
response, we create multi-lingual functional benchmarks -- Cross-Lingual Grade
School Math Symbolic (CL-GSM Symbolic) and Cross-Lingual Instruction-Following
Eval (CL-IFEval)-- by translating existing functional benchmark templates from
English to five additional languages that span the range of resources available
for NLP: French, Spanish, Hindi, Arabic and Yoruba. Our results reveal that
some static multi-lingual benchmarks capture functional performance much more
closely than others (i.e. across models, there is a 24%, 17% and 18% decrease
in performance between M-GSM and CL-GSM Symbolic in English, French and Spanish
respectively; similarly there's a 15 - 24% performance drop across languages
between Belebele and CL-IFEval, and only a 0.5% to 3% performance drop between
M-MMLU and CL-IFEval). Similarly, we find that model robustness across
languages varies significantly, with certain languages (eg. Arabic, English)
being the most consistently well performing across evaluation iterations.

</details>


### [3] [The Ideation-Execution Gap: Execution Outcomes of LLM-Generated versus Human Research Ideas](https://arxiv.org/abs/2506.20803)
*Chenglei Si,Tatsunori Hashimoto,Diyi Yang*

Main category: cs.CL

TL;DR: LLM生成的点子在执行后，其评价得分（新颖性、兴奋度、有效性和总体）显著低于人类专家点子，揭示了当前LLM在生成真正有效研究点子方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在加速科学研究和生成新颖研究点子方面展现出潜力，但本研究旨在验证AI生成的点子在实际执行后是否能带来更好的研究成果，以填补“点子新颖性”与“执行效果”之间的空白。

Method: 招募43位专家研究人员执行随机分配的研究点子（部分由专家撰写，部分由LLM生成）。每位专家投入超过100小时实现点子，并撰写4页短论文。所有执行的项目由NLP专家进行盲审，并比较点子在执行前后的评分变化。

Result: 与专家撰写的点子相比，LLM生成的点子在执行后，所有评估指标（新颖性、兴奋度、有效性和总体）的得分都显著下降，甚至在许多指标上，人类点子的排名高于LLM点子，弥补了最初在构思阶段观察到的LLM与人类点子之间的差距。

Conclusion: 当前LLM在生成真正有效的研究点子方面存在局限性，这突显了在缺乏执行结果的情况下评估研究点子所面临的挑战。

Abstract: Large Language Models (LLMs) have shown promise in accelerating the
scientific research pipeline. A key capability for this process is the ability
to generate novel research ideas, and prior studies have found settings in
which LLM-generated research ideas were judged as more novel than human-expert
ideas. However, a good idea should not simply appear to be novel, it should
also result in better research after being executed. To test whether
AI-generated ideas lead to better research outcomes, we conduct an execution
study by recruiting 43 expert researchers to execute randomly-assigned ideas,
either written by experts or generated by an LLM. Each expert spent over 100
hours implementing the idea and wrote a 4-page short paper to document the
experiments. All the executed projects are then reviewed blindly by expert NLP
researchers. Comparing the review scores of the same ideas before and after
execution, the scores of the LLM-generated ideas decrease significantly more
than expert-written ideas on all evaluation metrics (novelty, excitement,
effectiveness, and overall; p < 0.05), closing the gap between LLM and human
ideas observed at the ideation stage. When comparing the aggregated review
scores from the execution study, we even observe that for many metrics there is
a flip in rankings where human ideas score higher than LLM ideas. This
ideation-execution gap highlights the limitations of current LLMs in generating
truly effective research ideas and the challenge of evaluating research ideas
in the absence of execution outcomes.

</details>


### [4] [MultiFinRAG: An Optimized Multimodal Retrieval-Augmented Generation (RAG) Framework for Financial Question Answering](https://arxiv.org/abs/2506.20821)
*Chinmay Gondhalekar,Urjitkumar Patel,Fang-Chun Yeh*

Main category: cs.CL

TL;DR: MultiFinRAG是一个专门为金融问答设计的检索增强生成框架，它通过多模态提取和分层回退策略，克服了传统大语言模型在处理金融文档时的限制，在复杂金融问答任务上的准确率比ChatGPT-4o高出19%。


<details>
  <summary>Details</summary>
Motivation: 传统的LLMs和RAG在处理跨越多种模态（文本、表格、图像）的金融文档时，面临token限制、布局丢失和跨模态上下文碎片化的问题，难以进行联合推理，因此需要一个专门的解决方案。

Method: 本文引入了MultiFinRAG框架。该框架首先通过将表格和图像分组并发送给轻量级多模态LLM进行多模态提取，生成结构化JSON和简洁文本摘要。然后，这些输出与叙述性文本一起嵌入并以模态感知相似度阈值进行索引。最后，采用分层回退策略，根据需要动态升级上下文（从纯文本到文本+表格+图像），以实现跨模态推理。

Result: MultiFinRAG在包含文本、表格、图像和组合多模态推理的复杂金融问答任务上，比ChatGPT-4o（免费版）的准确率高出19个百分点。

Conclusion: MultiFinRAG是一个专为金融问答优化的检索增强生成框架，它有效地解决了现有方法在处理多模态金融文档时的局限性，并在实际应用中展现出显著的性能提升。

Abstract: Financial documents--such as 10-Ks, 10-Qs, and investor presentations--span
hundreds of pages and combine diverse modalities, including dense narrative
text, structured tables, and complex figures. Answering questions over such
content often requires joint reasoning across modalities, which strains
traditional large language models (LLMs) and retrieval-augmented generation
(RAG) pipelines due to token limitations, layout loss, and fragmented
cross-modal context. We introduce MultiFinRAG, a retrieval-augmented generation
framework purpose-built for financial QA. MultiFinRAG first performs multimodal
extraction by grouping table and figure images into batches and sending them to
a lightweight, quantized open-source multimodal LLM, which produces both
structured JSON outputs and concise textual summaries. These outputs, along
with narrative text, are embedded and indexed with modality-aware similarity
thresholds for precise retrieval. A tiered fallback strategy then dynamically
escalates from text-only to text+table+image contexts when necessary, enabling
cross-modal reasoning while reducing irrelevant context. Despite running on
commodity hardware, MultiFinRAG achieves 19 percentage points higher accuracy
than ChatGPT-4o (free-tier) on complex financial QA tasks involving text,
tables, images, and combined multimodal reasoning.

</details>


### [5] [Uncovering Hidden Violent Tendencies in LLMs: A Demographic Analysis via Behavioral Vignettes](https://arxiv.org/abs/2506.20822)
*Quintin Myers,Yanjun Gao*

Main category: cs.CL

TL;DR: 研究评估了LLM对日常暴力情景的反应，发现其表面输出与内部偏好不符，且暴力倾向因人口统计学特征而异，与人类行为研究结果相悖。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）被广泛提议用于在线暴力内容的检测和响应，但其在道德模糊的现实世界场景中进行推理的能力仍未得到充分检验。

Method: 本研究首次使用经过验证的社会科学工具“暴力行为情景问卷（VBVQ）”来评估LLM，该工具旨在衡量人类对日常冲突的反应。为了评估潜在偏见，引入了基于角色的提示，在美国境内改变种族、年龄和地理身份。在统一的零样本设置下，评估了来自不同地缘政治和组织背景的六个LLM。

Result: 研究揭示了两个关键发现：（1）LLM的表面文本生成与其对暴力反应的内部偏好常常存在差异；（2）它们的暴力倾向因人口统计学特征而异，这经常与犯罪学、社会科学和心理学中已有的研究结果相矛盾。

Conclusion: 大型语言模型在处理道德模糊的暴力情景时表现复杂且存在偏见，其行为模式与人类研究结果不符，提示其在实际应用中需要谨慎。

Abstract: Large language models (LLMs) are increasingly proposed for detecting and
responding to violent content online, yet their ability to reason about morally
ambiguous, real-world scenarios remains underexamined. We present the first
study to evaluate LLMs using a validated social science instrument designed to
measure human response to everyday conflict, namely the Violent Behavior
Vignette Questionnaire (VBVQ). To assess potential bias, we introduce
persona-based prompting that varies race, age, and geographic identity within
the United States. Six LLMs developed across different geopolitical and
organizational contexts are evaluated under a unified zero-shot setting. Our
study reveals two key findings: (1) LLMs surface-level text generation often
diverges from their internal preference for violent responses; (2) their
violent tendencies vary across demographics, frequently contradicting
established findings in criminology, social science, and psychology.

</details>


### [6] [Decide less, communicate more: On the construct validity of end-to-end fact-checking in medicine](https://arxiv.org/abs/2506.20876)
*Sebastian Joseph,Lily Chen,Barry Wei,Michael Mackert,Iain J. Marshall,Paul Pu Liang,Ramez Kouzy,Byron C. Wallace,Junyi Jessy Li*

Main category: cs.CL

TL;DR: 医学领域自动事实核查系统虽然有需求但未被充分利用。本研究通过观察临床专家如何核查社交媒体上的真实医学声明，揭示了端到端事实核查的根本挑战，如声明与证据连接困难、声明模糊性以及真实性标签的主观性。研究认为事实核查应被视为一个交互式沟通问题，而非简单的端到端过程。


<details>
  <summary>Details</summary>
Motivation: 尽管自动事实核查系统在公共卫生和医学领域的需求日益增长，但现有系统仍未被充分利用。由于医学决策的重要性以及普通用户难以理解浩瀚的医学文献，促使研究探索为何这些端到端事实核查系统未能广泛应用。

Method: 本研究首次考察了临床专家如何通过综合医学证据来核实社交媒体上的真实声明。

Result: 研究揭示了医学领域端到端事实核查面临的根本挑战：将“野外”声明与临床试验形式的科学证据联系起来的困难；不完全指定的声明与不匹配的意图混合导致的歧义；以及固有的主观真实性标签。

Conclusion: 研究认为，事实核查应被视为一个交互式的沟通问题来处理和评估，而不是一个简单的端到端过程。

Abstract: Technological progress has led to concrete advancements in tasks that were
regarded as challenging, such as automatic fact-checking. Interest in adopting
these systems for public health and medicine has grown due to the high-stakes
nature of medical decisions and challenges in critically appraising a vast and
diverse medical literature. Evidence-based medicine connects to every
individual, and yet the nature of it is highly technical, rendering the medical
literacy of majority users inadequate to sufficiently navigate the domain. Such
problems with medical communication ripens the ground for end-to-end
fact-checking agents: check a claim against current medical literature and
return with an evidence-backed verdict. And yet, such systems remain largely
unused. To understand this, we present the first study examining how clinical
experts verify real claims from social media by synthesizing medical evidence.
In searching for this upper-bound, we reveal fundamental challenges in
end-to-end fact-checking when applied to medicine: Difficulties connecting
claims in the wild to scientific evidence in the form of clinical trials;
ambiguities in underspecified claims mixed with mismatched intentions; and
inherently subjective veracity labels. We argue that fact-checking should be
approached and evaluated as an interactive communication problem, rather than
an end-to-end process.

</details>


### [7] [Optimising Language Models for Downstream Tasks: A Post-Training Perspective](https://arxiv.org/abs/2506.20917)
*Zhengyan Shi*

Main category: cs.CL

TL;DR: 该论文提出了一系列方法，旨在解决大型语言模型在适应特定任务时面临的挑战，如数据利用不足、过拟合和计算成本高昂等问题。其方法包括利用未标记数据、参数高效微调以及改进的监督微调，并引入了新的评估基准。研究结果表明，这些方法显著提高了语言模型的鲁棒性、效率和泛化能力，使其能更好地适应各种应用。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在NLP中表现出色，但将其有效且鲁棒地适应特定任务仍然具有挑战性。随着模型规模和复杂性的增长，在标记数据上进行微调常常未充分利用未标记数据，导致在小型任务特定数据集上过拟合，并产生显著的计算成本，这阻碍了它们在现实世界语言任务开放式环境中的应用。

Method: 本论文提出了一系列方法来更好地将语言模型应用于下游任务。首先，探索从无标记数据中提取任务相关知识的策略，引入了一种超越现有半监督方法的全新持续预训练技术。其次，提出了一种参数高效的微调方法，显著降低了内存和计算成本，同时保持了竞争力。此外，还引入了改进的监督微调方法，使语言模型能够更好地遵循指令，尤其是在标记数据稀缺时，从而提升其在一系列NLP任务（包括开放式生成）中的性能。最后，开发了新的评估方法和基准，例如多跳空间推理任务，以更全面地评估语言模型的能力和适应性。

Result: 通过对各种NLP任务进行广泛的实证研究，结果表明这些方法显著提高了语言模型的鲁棒性、效率和泛化能力，使其更适用于广泛的应用。

Conclusion: 这些进展标志着在构建更鲁棒、更高效的语言模型方面迈出了重要一步，使我们更接近通用人工智能的目标。

Abstract: Language models (LMs) have demonstrated remarkable capabilities in NLP, yet
adapting them efficiently and robustly to specific tasks remains challenging.
As their scale and complexity grow, fine-tuning LMs on labelled data often
underutilizes available unlabelled data, leads to overfitting on small
task-specific sets, and imposes significant computational costs. These
limitations hamper their application to the open-ended landscape of real-world
language tasks.
  This thesis proposes a series of methods to better adapt LMs to downstream
applications. First, we explore strategies for extracting task-relevant
knowledge from unlabelled data, introducing a novel continued pre-training
technique that outperforms state-of-the-art semi-supervised approaches. Next,
we present a parameter-efficient fine-tuning method that substantially reduces
memory and compute costs while maintaining competitive performance. We also
introduce improved supervised fine-tuning methods that enable LMs to better
follow instructions, especially when labelled data is scarce, enhancing their
performance across a range of NLP tasks, including open-ended generation.
Finally, we develop new evaluation methods and benchmarks, such as multi-hop
spatial reasoning tasks, to assess LM capabilities and adaptation more
comprehensively.
  Through extensive empirical studies across diverse NLP tasks, our results
demonstrate that these approaches substantially improve LM robustness,
efficiency, and generalization, making them more adaptable to a broad range of
applications. These advances mark a significant step towards more robust and
efficient LMs, bringing us closer to the goal of artificial general
intelligence.

</details>


### [8] [FineWeb2: One Pipeline to Scale Them All -- Adapting Pre-Training Data Processing to Every Language](https://arxiv.org/abs/2506.20920)
*Guilherme Penedo,Hynek Kydlíček,Vinko Sabolčec,Bettina Messmer,Negar Foroutan,Amir Hossein Kargaran,Colin Raffel,Martin Jaggi,Leandro Von Werra,Thomas Wolf*

Main category: cs.CL

TL;DR: 该论文介绍了一种可自动适应多语言的新型预训练数据集整理流程，并推出了FineWeb2数据集，解决了多语言LLM训练中数据质量和多样性的挑战。


<details>
  <summary>Details</summary>
Motivation: 目前高质量多语言大型语言模型（LLM）的训练面临挑战，主要原因在于难以针对大量语言定制过滤和去重流程，且缺乏大规模、高质量的非英语预训练数据集。

Method: 提出了一种基于FineWeb的新型预训练数据集整理流程，可自动适应任何语言。通过对九种不同语言的广泛消融实验，并结合基于可衡量标准的新型评估任务选择流程进行指导。此外，还引入了一种简单且有原则的数据集再平衡方法，考虑了重复计数和质量。

Result: 所提出的流程能够创建比现有数据集更能产生高性能模型的非英语语料库。数据集再平衡方法进一步提升了性能。该流程已扩展到超过1000种语言，使用近100个Common Crawl快照生成了FineWeb2，一个20TB（50亿文档）的新型多语言数据集。

Conclusion: 本研究成功开发并验证了一种新的数据集整理流程和FineWeb2数据集，有效解决了多语言LLM预训练中的数据质量和多样性挑战，显著提升了模型性能。

Abstract: Pre-training state-of-the-art large language models (LLMs) requires vast
amounts of clean and diverse text data. While the open development of large
high-quality English pre-training datasets has seen substantial recent
progress, training performant multilingual LLMs remains a challenge, in large
part due to the inherent difficulty of tailoring filtering and deduplication
pipelines to a large number of languages. In this work, we introduce a new
pre-training dataset curation pipeline based on FineWeb that can be
automatically adapted to support any language. We extensively ablate our
pipeline design choices on a set of nine diverse languages, guided by a set of
meaningful and informative evaluation tasks that were chosen through a novel
selection process based on measurable criteria. Ultimately, we show that our
pipeline can be used to create non-English corpora that produce more performant
models than prior datasets. We additionally introduce a straightforward and
principled approach to rebalance datasets that takes into consideration both
duplication count and quality, providing an additional performance uplift.
Finally, we scale our pipeline to over 1000 languages using almost 100 Common
Crawl snapshots to produce FineWeb2, a new 20 terabyte (5 billion document)
multilingual dataset which we release along with our pipeline, training, and
evaluation codebases.

</details>


### [9] [KaLM-Embedding-V2: Superior Training Techniques and Data Inspire A Versatile Embedding Model](https://arxiv.org/abs/2506.20923)
*Xinping Zhao,Xinshuo Hu,Zifei Shan,Shouzheng Huang,Yao Zhou,Zetian Sun,Zhenyu Liu,Dongfang Li,Xinyuan Wei,Qian Chen,Youcheng Pan,Yang Xiang,Meishan Zhang,Haofen Wang,Jun Yu,Baotian Hu,Min Zhang*

Main category: cs.CL

TL;DR: KaLM-Embedding-V2是一种紧凑且通用的文本嵌入模型，通过改进架构、多阶段训练（包括焦点重加权和在线难负样本混合）以及大量数据，在MTEB基准测试中显著优于同类模型，并能与大得多的模型竞争。


<details>
  <summary>Details</summary>
Motivation: 旨在提出一个通用且紧凑的嵌入模型，通过利用卓越的训练技术和数据，在通用文本嵌入任务中实现令人印象深刻的性能。

Method: 1. 架构改进：移除因果注意力掩码，采用全双向Transformer和简单的均值池化以生成固定长度嵌入。 2. 多阶段训练：i) 在大规模弱监督开源语料库上进行预训练；ii) 在高质量检索和非检索数据集上进行微调；iii) 使用模型汤参数平均实现鲁棒泛化。 3. 创新技术：引入焦点式重加权机制以集中学习困难样本，以及在线难负样本混合策略以持续丰富难负样本。 4. 数据收集：预训练收集超过20类数据，微调收集100类数据。

Result: 在MTEB中文和英文基准测试中，KaLM-Embedding-V2显著优于同等规模的其他模型，并能与大3倍、14倍、18倍和26倍的嵌入模型竞争，为参数小于1B的通用紧凑嵌入模型树立了新标准。

Conclusion: KaLM-Embedding-V2凭借卓越的训练技术和数据，成为一个高效、通用且紧凑的嵌入模型，实现了最先进的性能。

Abstract: In this paper, we propose KaLM-Embedding-V2, a versatile and compact
embedding model, which achieves impressive performance in general-purpose text
embedding tasks by leveraging superior training techniques and data. Our key
innovations include: (1) To better align the architecture with representation
learning, we remove the causal attention mask and adopt a fully bidirectional
transformer with simple yet effective mean-pooling to produce fixed-length
embeddings; (2) We employ a multi-stage training pipeline: (i) pre-training on
large-scale weakly supervised open-source corpora; (ii) fine-tuning on
high-quality retrieval and non-retrieval datasets; and (iii) model-soup
parameter averaging for robust generalization. Besides, we introduce a
focal-style reweighting mechanism that concentrates learning on difficult
samples and an online hard-negative mixing strategy to continuously enrich hard
negatives without expensive offline mining; (3) We collect over 20 categories
of data for pre-training and 100 categories of data for fine-tuning, to boost
both the performance and generalization of the embedding model. Extensive
evaluations on the Massive Text Embedding Benchmark (MTEB) Chinese and English
show that our model significantly outperforms others of comparable size, and
competes with 3x, 14x, 18x, and 26x larger embedding models, setting a new
standard for a versatile and compact embedding model with less than 1B
parameters.

</details>


### [10] [Can Gradient Descent Simulate Prompting?](https://arxiv.org/abs/2506.20989)
*Eric Zhang,Leshem Choshen,Jacob Andreas*

Main category: cs.CL

TL;DR: 该论文提出了一种元训练语言模型的方法，使其梯度更新能够模拟提示（prompting）的效果，从而使微调（fine-tuning）在某些任务上达到与提示相当的性能，并为长上下文建模和梯度泛化提供了新思路。


<details>
  <summary>Details</summary>
Motivation: 目前将新信息整合到语言模型中有两种主要方式：改变提示或改变参数（如通过微调）。提示在许多模型更新中更有效，可以从单个示例中鲁棒地泛化并进行逻辑推理；而参数更新没有长期存储成本。论文的动机是探讨能否修改模型，使得微调也能模拟提示的效果。

Method: 本文描述了一种元训练语言模型的方法，使得梯度更新能够模拟条件化新信息的效果。该方法利用了基于梯度的元学习工具，并使用语言模型自身的提示预测作为目标，从而无需真实标签。

Result: 随后的梯度下降训练恢复了部分（有时是全部）提示模型的性能——在“反转诅咒”任务上显示出改进，并在单次梯度更新后回答了有关文本段落的问题。

Conclusion: 这些结果表明，通过适当的初始化，梯度下降可以具有惊人的表达能力。我们的研究结果为长上下文建模提供了新的途径，并为基于梯度的学习的泛化能力提供了见解。

Abstract: There are two primary ways of incorporating new information into a language
model (LM): changing its prompt or changing its parameters, e.g. via
fine-tuning. Parameter updates incur no long-term storage cost for model
changes. However, for many model updates, prompting is significantly more
effective: prompted models can generalize robustly from single examples and
draw logical inferences that do not occur under standard fine-tuning. Can
models be modified so that fine-tuning does emulate prompting? This paper
describes a method for meta-training LMs such that gradient updates emulate the
effects of conditioning on new information. Our approach uses tools from
gradient-based meta-learning but uses an LM's own prompted predictions as
targets, eliminating the need for ground-truth labels. Subsequent gradient
descent training recovers some (and occasionally all) of prompted model
performance -- showing improvement on the ``reversal curse'' tasks, and
answering questions about text passages after a single gradient update. These
results suggest that, with appropriate initialization, gradient descent can be
surprisingly expressive. Our results suggest new avenues for long-context
modeling and offer insight into the generalization capabilities of
gradient-based learning.

</details>


### [11] [SAC: A Framework for Measuring and Inducing Personality Traits in LLMs with Dynamic Intensity Control](https://arxiv.org/abs/2506.20993)
*Adithya Chittem,Aishna Shrivastava,Sai Tarun Pendela,Jagat Sesh Challa,Dhruv Kumar*

Main category: cs.CL

TL;DR: 该研究扩展了LLM的个性化建模，通过16PF模型和SAC框架实现对16种特质及其强度的精细控制，使得LLM能展现更真实、更可控的人类般个性。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型（LLMs）个性化建模主要依赖粗糙的“大五”人格框架，且缺乏对特质强度进行控制的机制，无法满足人们对LLMs展现类人个性的期望。

Method: 本文通过将Machine Personality Inventory（MPI）扩展到16人格因素（16PF）模型，从而能够对16种不同的特质进行表达控制。此外，开发了结构化的特定属性控制（SAC）框架，用于评估和动态诱导LLMs的特质强度，引入基于形容词的语义锚定来指导特质强度表达，并利用跨越五个强度因素（频率、深度、阈值、努力、意愿）的行为问题。

Result: 实验发现，将强度建模为连续谱比二元特质切换能产生更一致和可控的个性表达。此外，目标特质强度的变化系统性地影响了密切相关的特质，表明LLMs内化了多维度人格结构。

Conclusion: 这项工作为医疗保健、教育和面试等领域受控且细致的人机交互开辟了新途径，使我们离真正的类人社交机器更近一步。

Abstract: Large language models (LLMs) have gained significant traction across a wide
range of fields in recent years. There is also a growing expectation for them
to display human-like personalities during interactions. To meet this
expectation, numerous studies have proposed methods for modelling LLM
personalities through psychometric evaluations. However, most existing models
face two major limitations: they rely on the Big Five (OCEAN) framework, which
only provides coarse personality dimensions, and they lack mechanisms for
controlling trait intensity. In this paper, we address this gap by extending
the Machine Personality Inventory (MPI), which originally used the Big Five
model, to incorporate the 16 Personality Factor (16PF) model, allowing
expressive control over sixteen distinct traits. We also developed a structured
framework known as Specific Attribute Control (SAC) for evaluating and
dynamically inducing trait intensity in LLMs. Our method introduces
adjective-based semantic anchoring to guide trait intensity expression and
leverages behavioural questions across five intensity factors:
\textit{Frequency}, \textit{Depth}, \textit{Threshold}, \textit{Effort}, and
\textit{Willingness}. Through experimentation, we find that modelling intensity
as a continuous spectrum yields substantially more consistent and controllable
personality expression compared to binary trait toggling. Moreover, we observe
that changes in target trait intensity systematically influence closely related
traits in psychologically coherent directions, suggesting that LLMs internalize
multi-dimensional personality structures rather than treating traits in
isolation. Our work opens new pathways for controlled and nuanced human-machine
interactions in domains such as healthcare, education, and interviewing
processes, bringing us one step closer to truly human-like social machines.

</details>


### [12] [Large Language Models Acing Chartered Accountancy](https://arxiv.org/abs/2506.21031)
*Jatin Gupta,Akhil Sharma,Saransh Singhania,Mohammad Adnan,Sakshi Deo,Ali Imam Abidi,Keshav Gupta*

Main category: cs.CL

TL;DR: 论文通过CA-Ben（基于印度特许会计师考试）评估了LLM在印度金融领域的表现。Claude 3.5 Sonnet和GPT-4o表现突出，但所有模型在数值计算和法律解释上都面临挑战。


<details>
  <summary>Details</summary>
Motivation: 旨在评估大型语言模型（LLM）在印度金融领域获取和应用专业知识的能力，因为其有效性尚不明确。

Method: 引入了CA-Ben，这是一个特许会计师基准测试，包含来自印度特许会计师协会（ICAI）考试的结构化问答数据集，用于评估LLM的金融、法律和定量推理能力。评估了六个主流LLM。

Result: Claude 3.5 Sonnet和GPT-4o表现最佳，尤其是在概念和法律推理方面。但在数值计算和法律解释方面存在显著挑战。

Conclusion: 论文强调了当前LLM的优缺点，并建议未来通过混合推理和检索增强生成方法进行改进，尤其是在定量分析和精确法律解释方面。

Abstract: Advanced intelligent systems, particularly Large Language Models (LLMs), are
significantly reshaping financial practices through advancements in Natural
Language Processing (NLP). However, the extent to which these models
effectively capture and apply domain-specific financial knowledge remains
uncertain. Addressing a critical gap in the expansive Indian financial context,
this paper introduces CA-Ben, a Chartered Accountancy benchmark specifically
designed to evaluate the financial, legal, and quantitative reasoning
capabilities of LLMs. CA-Ben comprises structured question-answer datasets
derived from the rigorous examinations conducted by the Institute of Chartered
Accountants of India (ICAI), spanning foundational, intermediate, and advanced
CA curriculum stages. Six prominent LLMs i.e. GPT 4o, LLAMA 3.3 70B, LLAMA 3.1
405B, MISTRAL Large, Claude 3.5 Sonnet, and Microsoft Phi 4 were evaluated
using standardized protocols. Results indicate variations in performance, with
Claude 3.5 Sonnet and GPT-4o outperforming others, especially in conceptual and
legal reasoning. Notable challenges emerged in numerical computations and legal
interpretations. The findings emphasize the strengths and limitations of
current LLMs, suggesting future improvements through hybrid reasoning and
retrieval-augmented generation methods, particularly for quantitative analysis
and accurate legal interpretation.

</details>


### [13] [A Semi-supervised Scalable Unified Framework for E-commerce Query Classification](https://arxiv.org/abs/2506.21049)
*Chunyuan Yuan,Chong Zhang,Zheng Fang,Ming Pang,Xue Jiang,Changping Peng,Zhangang Lin,Ching Law*

Main category: cs.CL

TL;DR: 论文提出半监督可扩展统一框架（SSUF），通过知识、标签和结构增强模块，解决电商查询分类中查询短、信息不足以及缺乏统一框架等问题，实验证明效果显著优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 电商查询分类（包括意图和类别预测）对电商应用至关重要，但面临挑战：查询通常短且缺乏上下文，标签间信息无法利用，导致建模的先验信息不足。现有方法多依赖用户后验点击行为构建训练样本，易产生马太效应。此外，查询分类的子任务缺乏统一框架，导致算法优化效率低下。

Method: 本文提出一种新颖的半监督可扩展统一框架（SSUF），包含多个增强模块以统一查询分类任务：知识增强模块利用世界知识增强查询表示，解决查询信息不足问题；标签增强模块利用标签语义和半监督信号，减少对后验标签的依赖；结构增强模块基于复杂的标签关系增强标签表示。每个模块高度可插拔，可根据子任务灵活添加或移除输入特征。

Result: 广泛的离线和在线A/B实验结果表明，SSUF显著优于现有最先进的模型。

Conclusion: SSUF成功地为电商查询分类任务提供了一个统一且高效的解决方案，通过引入知识、标签和结构增强，有效解决了短查询信息不足、依赖后验数据以及缺乏统一框架等关键挑战，并实现了显著的性能提升。

Abstract: Query classification, including multiple subtasks such as intent and category
prediction, is vital to e-commerce applications. E-commerce queries are usually
short and lack context, and the information between labels cannot be used,
resulting in insufficient prior information for modeling. Most existing
industrial query classification methods rely on users' posterior click behavior
to construct training samples, resulting in a Matthew vicious cycle.
Furthermore, the subtasks of query classification lack a unified framework,
leading to low efficiency for algorithm optimization.
  In this paper, we propose a novel Semi-supervised Scalable Unified Framework
(SSUF), containing multiple enhanced modules to unify the query classification
tasks. The knowledge-enhanced module uses world knowledge to enhance query
representations and solve the problem of insufficient query information. The
label-enhanced module uses label semantics and semi-supervised signals to
reduce the dependence on posterior labels. The structure-enhanced module
enhances the label representation based on the complex label relations. Each
module is highly pluggable, and input features can be added or removed as
needed according to each subtask. We conduct extensive offline and online A/B
experiments, and the results show that SSUF significantly outperforms the
state-of-the-art models.

</details>


### [14] [MT2-CSD: A New Dataset and Multi-Semantic Knowledge Fusion Method for Conversational Stance Detection](https://arxiv.org/abs/2506.21053)
*Fuqiang Niu,Genan Dai,Yisha Lu,Jiayu Liao,Xiang Li,Hu Huang,Bowen Zhang*

Main category: cs.CL

TL;DR: 本文提出了一个用于多目标、多轮对话立场检测的新数据集MT2-CSD，以及一个增强型会话关系注意力网络LLM-CRAN。LLM-CRAN在MT2-CSD数据集上表现优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 传统的立场检测研究通常针对单个实例，限制了其对真实社交媒体场景中多方对话建模的能力，这主要是由于缺乏真实捕捉社交媒体互动动态的数据集，从而阻碍了会话立场检测的进展。

Method: 本文引入了MT2-CSD，一个用于多目标、多轮会话立场检测的综合数据集，声称是该用途上最大的数据集。为了解决这些挑战，我们提出了大型语言模型增强型会话关系注意力网络（LLM-CRAN），该网络利用LLM的推理能力来提高会话理解。

Result: 实验结果表明，LLM-CRAN在会话立场检测任务中显著优于强大的基线模型。

Conclusion: 本文通过引入MT2-CSD数据集和LLM-CRAN模型，有效推进了多方、多轮会话立场检测的研究，并取得了显著的性能提升。

Abstract: In the realm of contemporary social media, automatic stance detection is
pivotal for opinion mining, as it synthesizes and examines user perspectives on
contentious topics to uncover prevailing trends and sentiments. Traditional
stance detection research often targets individual instances, thereby limiting
its capacity to model multi-party discussions typical in real social media
scenarios. This shortcoming largely stems from the scarcity of datasets that
authentically capture the dynamics of social media interactions, hindering
advancements in conversational stance detection. In this paper, we introduce
MT2-CSD, a comprehensive dataset for multi-target, multi-turn conversational
stance detection. To the best of our knowledge, MT2-CSD is the largest dataset
available for this purpose, comprising 24,457 annotated instances and
exhibiting the greatest conversational depth, thereby presenting new challenges
for stance detection. To address these challenges, we propose the Large
Language model enhanced Conversational Relational Attention Network (LLM-CRAN),
which exploits the reasoning capabilities of LLMs to improve conversational
understanding. We conduct extensive experiments to evaluate the efficacy of
LLM-CRAN on the MT2-CSD dataset. The experimental results indicate that
LLM-CRAN significantly outperforms strong baseline models in the task of
conversational stance detection.

</details>


### [15] [DALR: Dual-level Alignment Learning for Multimodal Sentence Representation Learning](https://arxiv.org/abs/2506.21096)
*Kang He,Yuzhe Ding. Haining Wang,Fei Li,Chong Teng,Donghong Ji*

Main category: cs.CL

TL;DR: DALR通过双层对齐学习解决多模态句子表示中的跨模态错位和模态内语义偏差问题，并在STS和TR任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态句子表示学习方法在图像和文本对齐时过于粗糙，导致跨模态错位偏差和模态内语义分歧，从而降低了句子表示的质量。

Method: 提出DALR（多模态句子表示的双层对齐学习）模型。通过一致性学习模块软化负样本并利用辅助任务的语义相似性实现细粒度跨模态对齐。此外，通过将排序蒸馏与全局模态内对齐学习相结合，更好地捕获句子间的排序关系，提升表示质量。

Result: 在语义文本相似性（STS）和迁移（TR）任务上的综合实验证明了该方法的有效性，并持续优于现有最先进的基线方法。

Conclusion: DALR有效解决了跨模态错位和模态内语义分歧的挑战，显著提升了多模态句子表示的质量。

Abstract: Previous multimodal sentence representation learning methods have achieved
impressive performance. However, most approaches focus on aligning images and
text at a coarse level, facing two critical challenges:cross-modal misalignment
bias and intra-modal semantic divergence, which significantly degrade sentence
representation quality. To address these challenges, we propose DALR
(Dual-level Alignment Learning for Multimodal Sentence Representation). For
cross-modal alignment, we propose a consistency learning module that softens
negative samples and utilizes semantic similarity from an auxiliary task to
achieve fine-grained cross-modal alignment. Additionally, we contend that
sentence relationships go beyond binary positive-negative labels, exhibiting a
more intricate ranking structure. To better capture these relationships and
enhance representation quality, we integrate ranking distillation with global
intra-modal alignment learning. Comprehensive experiments on semantic textual
similarity (STS) and transfer (TR) tasks validate the effectiveness of our
approach, consistently demonstrating its superiority over state-of-the-art
baselines.

</details>


### [16] [ComRAG: Retrieval-Augmented Generation with Dynamic Vector Stores for Real-time Community Question Answering in Industry](https://arxiv.org/abs/2506.21098)
*Qinwen Chen,Wenbiao Tao,Zhiwei Zhu,Mingfan Xi,Liangzhong Guo,Yuan Wang,Wei Wang,Yunshi Lan*

Main category: cs.CL

TL;DR: ComRAG是一个用于实时社区问答（CQA）的检索增强生成框架，它通过基于质心的记忆机制有效整合了静态知识和动态历史问答对，并在工业数据集上取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有CQA平台在实时利用历史交互和领域知识方面存在挑战，具体表现为未能充分利用外部知识、缺乏动态历史问答上下文以及不适合工业部署的记忆机制。

Method: 本文提出了ComRAG框架，一个用于实时工业CQA的检索增强生成框架。它通过一个为检索、生成和高效存储设计的基于质心的记忆机制，将静态知识与动态历史问答对相结合。

Result: ComRAG在三个工业CQA数据集上表现始终优于所有基线，向量相似度最高提升25.9%，延迟降低8.7%至23.3%，并且迭代过程中块增长从20.23%降至2.06%。

Conclusion: ComRAG框架成功解决了实时工业CQA中利用历史交互和领域知识的挑战，通过其创新的记忆机制显著提升了性能和效率。

Abstract: Community Question Answering (CQA) platforms can be deemed as important
knowledge bases in community, but effectively leveraging historical
interactions and domain knowledge in real-time remains a challenge. Existing
methods often underutilize external knowledge, fail to incorporate dynamic
historical QA context, or lack memory mechanisms suited for industrial
deployment. We propose ComRAG, a retrieval-augmented generation framework for
real-time industrial CQA that integrates static knowledge with dynamic
historical QA pairs via a centroid-based memory mechanism designed for
retrieval, generation, and efficient storage. Evaluated on three industrial CQA
datasets, ComRAG consistently outperforms all baselines--achieving up to 25.9%
improvement in vector similarity, reducing latency by 8.7% to 23.3%, and
lowering chunk growth from 20.23% to 2.06% over iterations.

</details>


### [17] [Progtuning: Progressive Fine-tuning Framework for Transformer-based Language Models](https://arxiv.org/abs/2506.21119)
*Xiaoshuang Ji,Zhendong Zhao,Xiaojun Chen,Xin Zhao,Zeyao Liu*

Main category: cs.CL

TL;DR: Progtuning是一种渐进式微调框架，通过根据Transformer块的贡献逐步减少更新的块数量，从而优化资源分配并显著减少更新参数（约25%），同时保持竞争力性能并与现有参数高效微调方法高度兼容。


<details>
  <summary>Details</summary>
Motivation: 现有微调方法（包括大多数参数高效微调方法）在更新大型Transformer模型时，需要更新固定数量的参数，忽略了不同Transformer块的贡献差异，导致计算资源分配效率低下且成本高昂。

Method: 本文提出了Progtuning，这是一种结合渐进式学习的新型微调框架，专门针对基于Transformer的语言模型。Progtuning通过根据Transformer块的贡献，逐步减少需要更新的Transformer块的数量。

Result: Progtuning优化了资源分配，将更新的参数数量减少了约25%，同时仍保持了具有竞争力的性能。此外，它还与参数高效微调方法表现出高度的适应性，在各种适应场景中展示了出色的性能。

Conclusion: Progtuning通过智能地减少更新的Transformer块数量，提供了一种高效且适应性强的Transformer模型微调解决方案，显著节约了资源并保持了高性能。

Abstract: Fine-tuning is a promising technique for leveraging Transformer-based
language models in downstream tasks. As model sizes continue to grow, updating
all model parameters becomes increasingly costly. Parameter-efficient
fine-tuning methods effectively address this issue by selectively updating a
small subset of parameters. However, fine-tuning and most existing
parameter-efficient fine-tuning methods require updating the same number of
parameters as the initial size, ignoring the unequal contribution across
Transformer blocks and leading to extremely inefficient allocation of computing
resources. In this paper, we propose Progtuning, the novel fine-tuning
framework combined with progressive learning for Transformer-based language
models. Specifically, Progtuning progressively reduces the number of updated
transformer blocks based on the contribution. Remarkably, Progtuning optimizes
resource allocation and reduces the number of updated parameters by
approximately 25\%, while still maintaining competitive performance. And it
also exhibits high adaptability with parameter-efficient fine-tuning methods,
demonstrating excellent performance across various adaptation scenarios.

</details>


### [18] [Compressed and Smooth Latent Space for Text Diffusion Modeling](https://arxiv.org/abs/2506.21170)
*Viacheslav Meshchaninov,Egor Chimbulatov,Alexander Shabalin,Aleksandr Abramov,Dmitry Vetrov*

Main category: cs.CL

TL;DR: 一种新的基于扩散的文本生成模型Cosmos，在一个压缩的潜在空间中工作，比自回归和其他扩散模型具有更好的质量和更快的推理速度。


<details>
  <summary>Details</summary>
Motivation: 自回归语言模型在文本生成中存在速度慢和全局连贯性差的限制。扩散模型虽然有潜力，但由于高维度的token级表示而难以应用于文本生成。

Method: 本文提出Cosmos，一种在为扩散量身定制的压缩、平滑潜在空间中进行文本生成的新方法。该空间通过一个自编码器学习，该自编码器同时进行token级重建并与预训练语言编码器的冻结激活对齐。

Result: 文本表示可以被压缩8倍，同时保持与token级扩散模型相当的生成质量。增加潜在序列长度使Cosmos超越了基于扩散和自回归的基线。Cosmos在四个生成任务中实现了相当或更优的生成质量，同时推理速度快2倍以上。

Conclusion: Cosmos在文本生成方面实现了与现有模型相当或更优的生成质量，并提供了显著的推理速度提升，有效解决了自回归模型的局限性和扩散模型在文本应用中的维度挑战。

Abstract: Autoregressive language models dominate modern text generation, yet their
sequential nature introduces fundamental limitations: decoding is slow, and
maintaining global coherence remains challenging. Diffusion models offer a
promising alternative by enabling parallel generation and flexible control;
however, their application to text generation is hindered by the high
dimensionality of token-level representations. We introduce Cosmos, a novel
approach to text generation that operates entirely in a compressed, smooth
latent space tailored specifically for diffusion. This space is learned using
an autoencoder trained simultaneously for token-level reconstruction and
alignment with frozen activations from a pretrained language encoder, providing
robust semantic grounding and enabling effective perturbation-based
augmentations. Empirically, we demonstrate that text representations can be
compressed by $8\times$ while maintaining generation quality comparable to
token-level diffusion models. Furthermore, increasing the latent sequence
length allows Cosmos to surpass both diffusion-based and autoregressive
baselines. We evaluate Cosmos on four diverse generative tasks including story
generation, question generation, summarization, and detoxification and compare
it with various generative paradigms. Cosmos achieves comparable or superior
generation quality while offering more than $2\times$ faster inference.

</details>


### [19] [Maintaining MTEB: Towards Long Term Usability and Reproducibility of Embedding Benchmarks](https://arxiv.org/abs/2506.21182)
*Isaac Chung,Imene Kerboua,Marton Kardos,Roman Solomatin,Kenneth Enevoldsen*

Main category: cs.CL

TL;DR: 本文探讨了大规模文本嵌入基准（MTEB）的工程实践，以确保其可复现性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 尽管MTEB的核心基准方法已经确立，但本文旨在通过工程实践，确保MTEB的持续可复现性和可扩展性。

Method: 作者介绍了维护稳健的持续集成管道的方法，包括验证数据集完整性、自动化测试执行和评估基准结果的泛化能力。文章还详细阐述了提高可复现性和可用性的设计选择，以及处理社区贡献和扩展基准的策略。

Result: 这些工程实践使得MTEB能够扩展得更全面，同时保持了质量和领域相关性。

Conclusion: 作者的经验为面临类似挑战的基准维护者提供了宝贵的见解，以确保机器学习评估框架中的可复现性和可用性。

Abstract: The Massive Text Embedding Benchmark (MTEB) has become a standard evaluation
platform for text embedding models. While previous work has established the
core benchmark methodology, this paper focuses on the engineering aspects that
ensure MTEB's continued reproducibility and extensibility. We present our
approach to maintaining robust continuous integration pipelines that validate
dataset integrity, automate test execution, and assess benchmark results'
generalizability. We detail the design choices that collectively enhance
reproducibility and usability. Furthermore, we discuss our strategies for
handling community contributions and extending the benchmark with new tasks and
datasets. These engineering practices have been instrumental in scaling MTEB to
become more comprehensive while maintaining quality and, ultimately, relevance
to the field. Our experiences offer valuable insights for benchmark maintainers
facing similar challenges in ensuring reproducibility and usability in machine
learning evaluation frameworks. The MTEB repository is available at:
https://github.com/embeddings-benchmark/mteb

</details>


### [20] [Prompt-Guided Turn-Taking Prediction](https://arxiv.org/abs/2506.21191)
*Koji Inoue,Mikey Elmers,Yahui Fu,Zi Haur Pang,Divesh Lala,Keiko Ochi,Tatsuya Kawahara*

Main category: cs.CL

TL;DR: 一个基于Transformer的新模型，允许通过文本提示（如“更快”或“更平静”）动态控制对话系统中的轮流预测，显示出更高的准确性和适应性。


<details>
  <summary>Details</summary>
Motivation: 在对话系统中，轮流预测模型至关重要，而本文旨在提出一种新颖的模型，通过文本提示实现轮流预测的动态控制，以更好地适应对话伙伴和上下文。

Method: 提出了一种新模型，在基于Transformer的语音活动预测（VAP）模型的基础上，将文本提示嵌入整合到通道内Transformer和跨通道Transformer中。由于缺乏现有数据，使用了大型语言模型（LLM）生成合成提示语句。

Result: 实验结果表明，所提出的模型提高了预测准确性，并根据文本提示有效地改变了轮流时序行为。

Conclusion: 该模型成功地实现了通过文本提示动态控制轮流预测，从而提高了准确性并使时序行为具有适应性。

Abstract: Turn-taking prediction models are essential components in spoken dialogue
systems and conversational robots. Recent approaches leverage transformer-based
architectures to predict speech activity continuously and in real-time. In this
study, we propose a novel model that enables turn-taking prediction to be
dynamically controlled via textual prompts. This approach allows intuitive and
explicit control through instructions such as "faster" or "calmer" adapting
dynamically to conversational partners and contexts. The proposed model builds
upon a transformer-based voice activity projection (VAP) model, incorporating
textual prompt embeddings into both channel-wise transformers and a
cross-channel transformer. We evaluated the feasibility of our approach using
over 950 hours of human-human spoken dialogue data. Since textual prompt data
for the proposed approach was not available in existing datasets, we utilized a
large language model (LLM) to generate synthetic prompt sentences. Experimental
results demonstrated that the proposed model improved prediction accuracy and
effectively varied turn-taking timing behaviors according to the textual
prompts.

</details>


### [21] [Enhancing Automatic Term Extraction with Large Language Models via Syntactic Retrieval](https://arxiv.org/abs/2506.21222)
*Yongchan Chun,Minhyuk Kim,Dongjun Kim,Chanjun Park,Heuiseok Lim*

Main category: cs.CL

TL;DR: 该论文提出了一种基于检索的提示策略，通过语法而非语义相似性为LLM选择示例，从而提高术语提取的F1分数。


<details>
  <summary>Details</summary>
Motivation: 自动术语提取（ATE）对于机器翻译和信息检索等下游任务至关重要。尽管大型语言模型（LLMs）在各种自然语言处理任务中取得了显著进展，但其在ATE方面的潜力却很少被研究。

Method: 我们提出了一种基于检索的提示策略，在少量样本设置下，根据语法而非语义相似性选择示例。这种句法检索方法是领域无关的，并为捕获术语边界提供了更可靠的指导。

Result: 在三个专门的ATE基准测试上的实验表明，句法检索提高了F1分数。我们还分析了查询句子与其检索到的示例之间的词汇重叠如何影响性能。

Conclusion: 这些发现强调了在将LLMs应用于术语提取任务时语法线索的重要性。

Abstract: Automatic Term Extraction (ATE) identifies domain-specific expressions that
are crucial for downstream tasks such as machine translation and information
retrieval. Although large language models (LLMs) have significantly advanced
various NLP tasks, their potential for ATE has scarcely been examined. We
propose a retrieval-based prompting strategy that, in the few-shot setting,
selects demonstrations according to \emph{syntactic} rather than semantic
similarity. This syntactic retrieval method is domain-agnostic and provides
more reliable guidance for capturing term boundaries. We evaluate the approach
in both in-domain and cross-domain settings, analyzing how lexical overlap
between the query sentence and its retrieved examples affects performance.
Experiments on three specialized ATE benchmarks show that syntactic retrieval
improves F1-score. These findings highlight the importance of syntactic cues
when adapting LLMs to terminology-extraction tasks.

</details>


### [22] [Agent-RewardBench: Towards a Unified Benchmark for Reward Modeling across Perception, Planning, and Safety in Real-World Multimodal Agents](https://arxiv.org/abs/2506.21252)
*Tianyi Men,Zhuoran Jin,Pengfei Cao,Yubo Chen,Kang Liu,Jun Zhao*

Main category: cs.CL

TL;DR: 多模态大语言模型（MLLMs）在代理任务中缺乏外部反馈，难以自我纠正和泛化。本文提出了Agent-RewardBench，一个用于评估MLLM奖励建模能力的基准，涵盖多维度、步级评估和高质量数据。实验表明，当前SOTA模型表现有限，凸显了代理奖励建模领域专业训练的必要性。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型（MLLMs）在实际任务中因缺乏外部反馈而难以进行自我纠正和泛化。尽管奖励模型是一个有前景的外部反馈方式，但目前尚不清楚如何为代理选择合适的奖励模型，因此迫切需要为代理构建一个奖励基准。

Method: 本文提出了Agent-RewardBench，一个旨在评估MLLMs奖励建模能力的基准。该基准具有三个主要特点：1) 在感知、规划和安全等7个真实世界代理场景中进行多维度评估；2) 提供步级奖励评估，以更细致地查看规划过程中的性能；3) 通过从10个多样化模型中采样、难度控制和人工验证来确保适当的难度和高质量数据。

Result: 实验结果表明，即使是最先进的多模态模型也表现有限，这突出表明了在代理奖励建模方面需要专门的训练。

Conclusion: Agent-RewardBench的实验结果表明，当前的多模态模型在代理奖励建模方面表现不佳，因此迫切需要针对这一领域进行专门的训练。

Abstract: As Multimodal Large Language Models (MLLMs) advance, multimodal agents show
promise in real-world tasks like web navigation and embodied intelligence.
However, due to limitations in a lack of external feedback, these agents
struggle with self-correction and generalization. A promising approach is to
use reward models as external feedback, but there is no clear on how to select
reward models for agents. Thus, there is an urgent need to build a reward bench
targeted at agents. To address these challenges, we propose Agent-RewardBench,
a benchmark designed to evaluate reward modeling ability in MLLMs. The
benchmark is characterized by three key features: (1) Multiple dimensions and
real-world agent scenarios evaluation. It covers perception, planning, and
safety with 7 scenarios; (2) Step-level reward evaluation. It allows for the
assessment of agent capabilities at the individual steps of a task, providing a
more granular view of performance during the planning process; and (3)
Appropriately difficulty and high-quality. We carefully sample from 10 diverse
models, difficulty control to maintain task challenges, and manual verification
to ensure the integrity of the data. Experiments demonstrate that even
state-of-the-art multimodal models show limited performance, highlighting the
need for specialized training in agent reward modeling. Code is available at
github.

</details>


### [23] [Cat and Mouse -- Can Fake Text Generation Outpace Detector Systems?](https://arxiv.org/abs/2506.21274)
*Andrea McGlinchey,Peter J Barclay*

Main category: cs.CL

TL;DR: 大型语言模型能生成逼真的“假文本”。本文研究了模型规避检测的能力是否会达到平台期。发现Gemini在生成欺骗性文本方面的能力有所提升，而GPT没有。这表明即使是更大的模型，可靠的检测仍然可行。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型可以生成令人信服的“假文本”，引发了人们对检测“假文本”能力是否会陷入无休止的“军备竞赛”的担忧。本文旨在探讨模型规避检测的能力是否会达到平台期。

Method: 本文通过统计分类器来识别古典侦探小说风格的“假文本”，并比较了Gemini和GPT在0.5版本更新后的表现。

Result: Gemini在生成欺骗性文本方面的能力有所提升，而GPT没有。

Conclusion: 即使是更大规模的模型，可靠的假文本检测可能仍然可行，尽管新的模型架构可能会提高其欺骗性。

Abstract: Large language models can produce convincing "fake text" in domains such as
academic writing, product reviews, and political news. Many approaches have
been investigated for the detection of artificially generated text. While this
may seem to presage an endless "arms race", we note that newer LLMs use ever
more parameters, training data, and energy, while relatively simple classifiers
demonstrate a good level of detection accuracy with modest resources. To
approach the question of whether the models' ability to beat the detectors may
therefore reach a plateau, we examine the ability of statistical classifiers to
identify "fake text" in the style of classical detective fiction. Over a 0.5
version increase, we found that Gemini showed an increased ability to generate
deceptive text, while GPT did not. This suggests that reliable detection of
fake text may remain feasible even for ever-larger models, though new model
architectures may improve their deceptiveness

</details>


### [24] [Double-Checker: Enhancing Reasoning of Slow-Thinking LLMs via Self-Critical Fine-Tuning](https://arxiv.org/abs/2506.21285)
*Xin Xu,Tianhao Chen,Fan Zhang,Wanlong Liu,Pengxiang Li,Ajay Kumar Jaiswal,Yuchen Yan,Jishan Hu,Yang Wang,Hao Chen,Shiwei Liu,Shizhe Diao,Can Yang,Lu Yin*

Main category: cs.CL

TL;DR: Double-Checker通过自我批判和迭代优化，显著提升了慢思考LLM的推理能力。


<details>
  <summary>Details</summary>
Motivation: 慢思考LLM的反射式推理能力在生成批判和优化现有解决方案方面仍有限。

Method: 引入Double-Checker框架，通过在1,730个自我批判实例上进行微调，使LLM在推理过程中能迭代地批判和优化输出。

Result: 迭代自我批判显著增强了长CoT LLM的推理能力，在AIME基准测试中，Double-Checker的pass@1性能从4.4%提升到18.2%。

Conclusion: 该研究为开发更可信、有效的具备结构化自我批判能力的LLM指明了方向。

Abstract: While slow-thinking large language models (LLMs) exhibit reflection-like
reasoning, commonly referred to as the "aha moment:, their ability to generate
informative critiques and refine prior solutions remains limited. In this
paper, we introduce Double-Checker, a principled framework designed to enhance
the reasoning capabilities of slow-thinking LLMs by fostering explicit
self-critique and iterative refinement of their previous solutions. By
fine-tuning on our curated 1,730 self-critical instances, Double-Checker
empowers long-CoT LLMs to iteratively critique and refine their outputs during
inference until they evaluate their solutions as correct under self-generated
critiques. We validate the efficacy of Double-Checker across a comprehensive
suite of reasoning benchmarks, demonstrating that iterative self-critique
significantly enhances the reasoning capabilities of long-CoT LLMs. Notably,
our Double-Checker increases the pass@1 performance on challenging AIME
benchmarks from 4.4% to 18.2% compared to the original long-CoT LLMs. These
results highlight a promising direction for developing more trustworthy and
effective LLMs capable of structured self-critique.

</details>


### [25] [Small Encoders Can Rival Large Decoders in Detecting Groundedness](https://arxiv.org/abs/2506.21288)
*Istabrak Abbes,Gabriele Prato,Quentin Fournier,Fernando Rodriguez,Alaa Boukhary,Adam Elwood,Sarath Chandar*

Main category: cs.CL

TL;DR: 该研究提出在大型语言模型（LLMs）生成答案前，使用轻量级模型（如RoBERTa和NomicBERT）检测查询是否基于提供上下文，从而在保持准确性的同时大幅降低推理时间和资源消耗。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在处理自然语言任务时，通过外部上下文能显著提升性能。然而，当上下文信息不足时，LLMs常会进行无根据的推测或依赖内部知识，导致答案不可靠。为确保事实一致性和可信度，生成严格基于上下文的回答（即“接地性”）至关重要。本研究旨在LLMs进行昂贵的答案生成之前，检测给定查询是否基于提供的文档，以减少推理时间和资源消耗。

Method: 本研究关注于在LLMs生成答案前，检测查询是否“接地”（即严格基于上下文）。具体方法是利用轻量级的、针对特定任务的编码器模型，如RoBERTa和NomicBERT，并在精选的数据集上进行微调。

Result: 轻量级的、特定任务的编码器模型（如RoBERTa和NomicBERT），在接地性检测方面的准确率与最先进的LLMs（如Llama3 8B和GPT4o）相当。同时，这些轻量级模型将推理延迟降低了数量级。

Conclusion: 在大型语言模型生成答案之前，使用轻量级模型进行查询接地性检测，可以在不牺牲准确性的前提下显著提高效率，确保LLM输出更可靠和值得信赖。

Abstract: Augmenting large language models (LLMs) with external context significantly
improves their performance in natural language processing (NLP) tasks. However,
LLMs struggle to answer queries reliably when the provided context lacks
information, often resorting to ungrounded speculation or internal knowledge.
Groundedness - generating responses strictly supported by the context - is
essential for ensuring factual consistency and trustworthiness. This study
focuses on detecting whether a given query is grounded in a document provided
in context before the costly answer generation by LLMs. Such a detection
mechanism can significantly reduce both inference time and resource
consumption. We show that lightweight, task specific encoder models such as
RoBERTa and NomicBERT, fine-tuned on curated datasets, can achieve accuracy
comparable to state-of-the-art LLMs, such as Llama3 8B and GPT4o, in
groundedness detection while reducing inference latency by orders of magnitude.
The code is available at : https://github.com/chandarlab/Hallucinate-less

</details>


### [26] [Detecting Referring Expressions in Visually Grounded Dialogue with Autoregressive Language Models](https://arxiv.org/abs/2506.21294)
*Bram Willemsen,Gabriel Skantze*

Main category: cs.CL

TL;DR: 本文探讨了仅使用文本的自回归语言模型从视觉接地对话中提取指称表达。研究发现，即使是中等规模的LLM、小型数据集和参数高效微调，纯文本方法也能有效，但该任务本质上是多模态的，单模态方法存在局限性。


<details>
  <summary>Details</summary>
Motivation: 探究仅通过语言上下文能在多大程度上识别视觉接地对话中的指称表达。

Method: 采用预训练大型语言模型（LLM）进行文本指称范围的粗粒度标注，通过预测下一个token来划定边界。

Result: 研究表明，即使使用中等规模的LLM、相对较小的数据集和参数高效微调，纯文本方法也能有效，突出了语言上下文对于该任务的相对重要性。

Conclusion: 该任务本质上是一个多模态问题，单模态方法存在固有的局限性。

Abstract: In this paper, we explore the use of a text-only, autoregressive language
modeling approach for the extraction of referring expressions from visually
grounded dialogue. More specifically, the aim is to investigate the extent to
which the linguistic context alone can inform the detection of mentions that
have a (visually perceivable) referent in the visual context of the
conversation. To this end, we adapt a pretrained large language model (LLM) to
perform a relatively course-grained annotation of mention spans in unfolding
conversations by demarcating mention span boundaries in text via next-token
prediction. Our findings indicate that even when using a moderately sized LLM,
relatively small datasets, and parameter-efficient fine-tuning, a text-only
approach can be effective, highlighting the relative importance of the
linguistic context for this task. Nevertheless, we argue that the task
represents an inherently multimodal problem and discuss limitations fundamental
to unimodal approaches.

</details>


### [27] [Structuralist Approach to AI Literary Criticism: Leveraging Greimas Semiotic Square for Large Language Models](https://arxiv.org/abs/2506.21360)
*Fangzhou Dong,Yifan Zeng,Yingpeng Sang,Hong Shen*

Main category: cs.CL

TL;DR: 本文提出了GLASS框架，基于格雷马斯符号方阵，旨在提升大型语言模型进行深度文学分析的能力。该框架表现出色，为文学研究和教育提供了一个新的AI工具。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在理解和生成文本方面表现出色，但在为思想深刻、叙事复杂的作品提供专业的文学批评方面存在困难。

Method: 提出了GLASS（Greimas Literary Analysis via Semiotic Square）框架，这是一个基于格雷马斯符号方阵（GSS）的结构化分析框架，以增强大型语言模型进行深度文学分析的能力。构建了首个基于GSS的文学批评数据集，包含48部作品的详细分析。提出了使用“大型语言模型作为评判者”范式的GSS文学批评定量指标。

Result: GLASS框架的结果在多部作品和多个大型语言模型上与专家批评进行比较，显示出高性能。将GLASS应用于39部经典作品，产生了原创且高质量的分析，填补了现有研究空白。

Conclusion: 这项研究为文学研究和教育提供了一个基于人工智能的工具，为文学参与背后的认知机制提供了见解。

Abstract: Large Language Models (LLMs) excel in understanding and generating text but
struggle with providing professional literary criticism for works with profound
thoughts and complex narratives. This paper proposes GLASS (Greimas Literary
Analysis via Semiotic Square), a structured analytical framework based on
Greimas Semiotic Square (GSS), to enhance LLMs' ability to conduct in-depth
literary analysis. GLASS facilitates the rapid dissection of narrative
structures and deep meanings in narrative works. We propose the first dataset
for GSS-based literary criticism, featuring detailed analyses of 48 works. Then
we propose quantitative metrics for GSS-based literary criticism using the
LLM-as-a-judge paradigm. Our framework's results, compared with expert
criticism across multiple works and LLMs, show high performance. Finally, we
applied GLASS to 39 classic works, producing original and high-quality analyses
that address existing research gaps. This research provides an AI-based tool
for literary research and education, offering insights into the cognitive
mechanisms underlying literary engagement.

</details>


### [28] [Leveraging LLM-Assisted Query Understanding for Live Retrieval-Augmented Generation](https://arxiv.org/abs/2506.21384)
*Guanting Dong,Xiaoxi Li,Yuyao Zhang,Mengjie Deng*

Main category: cs.CL

TL;DR: Omni-RAG是一个新的RAG框架，通过LLM辅助的查询理解、多意图分解、意图感知检索和重排序来处理复杂和嘈杂的实时用户查询，旨在提高RAG在真实世界应用中的鲁棒性和有效性。


<details>
  <summary>Details</summary>
Motivation: 实时检索增强生成（RAG）系统在处理嘈杂、模糊和包含多重意图的用户查询时面临重大挑战，因为当前系统通常在更干净的数据上进行训练或评估。

Method: 本文提出了Omni-RAG，一个新颖的框架，旨在提高RAG系统在实时、开放域设置中的鲁棒性和有效性。Omni-RAG采用LLM辅助的查询理解来预处理用户输入，通过三个关键模块：1) 深度查询理解与分解（利用LLM去噪并分解多意图查询）；2) 意图感知知识检索（对每个子查询执行检索并聚合结果）；以及3) 重排序和生成（使用重排序器优化文档选择，然后由LLM生成最终响应）。

Result: Omni-RAG旨在通过强大地处理复杂和嘈杂的查询，弥合当前RAG能力与真实世界应用需求（如SIGIR 2025 LiveRAG挑战赛所强调的）之间的差距。

Conclusion: Omni-RAG是一个为提高RAG系统在实时、开放域环境下的鲁棒性和有效性而设计的新颖框架，通过有效处理复杂和嘈杂的查询来满足真实世界的应用需求。

Abstract: Real-world live retrieval-augmented generation (RAG) systems face significant
challenges when processing user queries that are often noisy, ambiguous, and
contain multiple intents. While RAG enhances large language models (LLMs) with
external knowledge, current systems typically struggle with such complex
inputs, as they are often trained or evaluated on cleaner data. This paper
introduces Omni-RAG, a novel framework designed to improve the robustness and
effectiveness of RAG systems in live, open-domain settings. Omni-RAG employs
LLM-assisted query understanding to preprocess user inputs through three key
modules: (1) Deep Query Understanding and Decomposition, which utilizes LLMs
with tailored prompts to denoise queries (e.g., correcting spelling errors) and
decompose multi-intent queries into structured sub-queries; (2) Intent-Aware
Knowledge Retrieval, which performs retrieval for each sub-query from a corpus
(i.e., FineWeb using OpenSearch) and aggregates the results; and (3) Reranking
and Generation, where a reranker (i.e., BGE) refines document selection before
a final response is generated by an LLM (i.e., Falcon-10B) using a
chain-of-thought prompt. Omni-RAG aims to bridge the gap between current RAG
capabilities and the demands of real-world applications, such as those
highlighted by the SIGIR 2025 LiveRAG Challenge, by robustly handling complex
and noisy queries.

</details>


### [29] [Domain Knowledge-Enhanced LLMs for Fraud and Concept Drift Detection](https://arxiv.org/abs/2506.21443)
*Ali Şenol,Garima Agrawal,Huan Liu*

Main category: cs.CL

TL;DR: 该研究提出了一个领域知识（DK）增强的LLM框架，用于检测欺骗性对话和概念漂移。通过整合预训练LLM和特定任务洞察，该框架能高精度识别虚假对话并有效分类漂移性质，显著提升了高风险NLP应用的性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在动态平台上检测欺骗性对话因语言模式演变和概念漂移（CD）而日益困难。尽管大型语言模型（LLM）在自然语言任务中表现出色，但在风险敏感场景中常面临上下文歧义和幻觉问题。

Method: 本文提出了一个领域知识（DK）增强的LLM框架，用于欺诈和概念漂移检测。该架构包含三个主要组件：一个DK-LLM模块用于检测虚假或欺骗性对话；一个漂移检测单元（OCDD）用于判断是否发生语义漂移；以及第二个DK-LLM模块用于将漂移分类为良性或欺诈性。

Result: 结果表明，该系统能高精度检测虚假对话并有效分类漂移性质。基于LLaMA的实现通过结构化提示达到了98%的分类准确率。对比零样本基线研究表明，结合领域知识和漂移感知显著提高了高风险NLP应用的性能、可解释性和鲁棒性。

Conclusion: 将领域知识和漂移感知相结合，能显著提高高风险NLP应用的性能、可解释性和鲁棒性。

Abstract: Detecting deceptive conversations on dynamic platforms is increasingly
difficult due to evolving language patterns and Concept Drift (CD)-i.e.,
semantic or topical shifts that alter the context or intent of interactions
over time. These shifts can obscure malicious intent or mimic normal dialogue,
making accurate classification challenging. While Large Language Models (LLMs)
show strong performance in natural language tasks, they often struggle with
contextual ambiguity and hallucinations in risk-sensitive scenarios. To address
these challenges, we present a Domain Knowledge (DK)-Enhanced LLM framework
that integrates pretrained LLMs with structured, task-specific insights to
perform fraud and concept drift detection. The proposed architecture consists
of three main components: (1) a DK-LLM module to detect fake or deceptive
conversations; (2) a drift detection unit (OCDD) to determine whether a
semantic shift has occurred; and (3) a second DK-LLM module to classify the
drift as either benign or fraudulent. We first validate the value of domain
knowledge using a fake review dataset and then apply our full framework to
SEConvo, a multiturn dialogue dataset that includes various types of fraud and
spam attacks. Results show that our system detects fake conversations with high
accuracy and effectively classifies the nature of drift. Guided by structured
prompts, the LLaMA-based implementation achieves 98% classification accuracy.
Comparative studies against zero-shot baselines demonstrate that incorporating
domain knowledge and drift awareness significantly improves performance,
interpretability, and robustness in high-stakes NLP applications.

</details>


### [30] [Text2Cypher Across Languages: Evaluating Foundational Models Beyond English](https://arxiv.org/abs/2506.21445)
*Makbule Gulcin Ozsoy,William Tai*

Main category: cs.CL

TL;DR: 该研究评估了多语言Text2Cypher的性能，发现英语表现最佳，其次是西班牙语，土耳其语最差，这与训练数据和语言特性有关；提示翻译影响很小。强调了多语言查询生成中更具包容性评估的需求。


<details>
  <summary>Details</summary>
Motivation: 目前大型语言模型在自然语言数据库查询接口（如Text2SQL、Text2SPARQL、Text2Cypher）方面的研究主要集中在英语，对其他语言的评估有限。

Method: 通过将英语问题翻译成西班牙语和土耳其语，并保留原始Cypher查询，创建并发布了一个多语言测试集。使用标准化提示和指标评估了多个基础模型在Text2Cypher任务上的表现。

Result: 模型性能表现出一致的模式：英语最高，其次是西班牙语，土耳其语最低，这归因于训练数据可用性和语言特征的差异。将任务提示翻译成西班牙语和土耳其语对评估指标影响很小。

Conclusion: 研究结果强调了在多语言查询生成中进行更具包容性的评估和开发的必要性。未来的工作包括模式本地化和跨不同语言的微调。

Abstract: Recent advances in large language models have enabled natural language
interfaces that translate user questions into database queries, such as
Text2SQL, Text2SPARQL, and Text2Cypher. While these interfaces enhance database
accessibility, most research today focuses solely on English, with limited
evaluation in other languages. This paper investigates the performance of
foundational LLMs on the Text2Cypher task across multiple languages. We create
and release a multilingual test set by translating English questions into
Spanish and Turkish while preserving the original Cypher queries, enabling fair
cross-lingual comparison. We evaluate multiple foundational models using
standardized prompts and metrics. Our results show a consistent performance
pattern: highest on English, then Spanish, and lowest on Turkish. We attribute
this to differences in training data availability and linguistic
characteristics. Additionally, we explore the impact of translating task
prompts into Spanish and Turkish. Results show little to no change in
evaluation metrics, suggesting prompt translation has minor impact. Our
findings highlight the need for more inclusive evaluation and development in
multilingual query generation. Future work includes schema localization and
fine-tuning across diverse languages.

</details>


### [31] [Aligning Spoken Dialogue Models from User Interactions](https://arxiv.org/abs/2506.21463)
*Anne Wu,Laurent Mazaré,Neil Zeghidour,Alexandre Défossez*

Main category: cs.CL

TL;DR: 该论文提出了一种新的偏好对齐框架，用于改进实时对话中的口语对话模型。通过创建一个包含15万多对偏好的大规模数据集，并利用离线对齐方法微调全双工语音到语音模型，该研究显著提升了口语对话模型的真实性、安全性和上下文对齐能力，强调了在实时语音对话系统中平衡各种动态的重要性。


<details>
  <summary>Details</summary>
Motivation: 当前的偏好学习方法主要关注基于文本的语言模型，不适用于实时语音交互的复杂性，例如中断、插话以及说话者轮次之间没有明确的分割。

Method: 1. 创建了一个包含超过15万个偏好对的大规模数据集，这些数据来源于原始多轮语音对话，并用AI反馈进行标注，以涵盖语言内容和时间上下文变化的偏好。
2. 利用离线对齐方法对全双工自回归语音到语音模型进行微调。

Result: 1. 大量实验表明，对通用对话的反馈可以持续有效地改进口语对话模型，使其产生更真实、更安全、更符合上下文的交互。
2. 部署了微调后的模型并进行了整体人工评估，以评估其在单轮对话之外的影响。

Conclusion: 研究结果揭示了在各种动态之间实现良好校准平衡的重要性，这对于自然的实时语音对话系统至关重要。

Abstract: We propose a novel preference alignment framework for improving spoken
dialogue models on real-time conversations from user interactions. Current
preference learning methods primarily focus on text-based language models, and
are not directly suited to the complexities of real-time speech interactions,
with richer dynamics (e.g. interruption, interjection) and no explicit
segmentation between speaker turns.We create a large-scale dataset of more than
150,000 preference pairs from raw multi-turn speech conversations, annotated
with AI feedback, to cover preferences over both linguistic content and
temporal context variations. We leverage offline alignment methods to finetune
a full-duplex autoregressive speech-to-speech model. Extensive experiments
demonstrate that feedback on generic conversations can be consistently
effective in improving spoken dialogue models to produce more factual, safer
and more contextually aligned interactions. We deploy the finetuned model and
conduct holistic human evaluations to assess the impact beyond single-turn
conversations. Our findings shed light on the importance of a well-calibrated
balance among various dynamics, crucial for natural real-time speech dialogue
systems.

</details>


### [32] [TopK Language Models](https://arxiv.org/abs/2506.21468)
*Ryosuke Takahashi,Tatsuro Inaba,Kentaro Inui,Benjamin Heinzerling*

Main category: cs.CL

TL;DR: 本文提出TopK LMs，通过在Transformer中引入TopK激活函数，解决了稀疏自编码器（SAE）在语言模型可解释性方面的局限性。TopK LMs无需事后训练，提供稳定的可解释性，并能成功进行神经元干预，有助于理解模型学习过程。


<details>
  <summary>Details</summary>
Motivation: 稀疏自编码器（SAEs）在分析和解释Transformer语言模型时存在局限性，例如：不清楚未能发现特定概念是SAE的问题还是底层语言模型的问题；训练条件和架构选择会影响SAE学习到的特征；特征缺乏稳定性导致难以比较不同检查点间的SAE特征。

Method: 通过修改Transformer架构，在选定层中引入TopK激活函数，使模型的隐藏状态等同于TopK SAE的潜在特征，从而无需事后训练。

Result: TopK LMs在模型大小、计算效率和可解释性之间提供了良好的权衡，并保持了其原始能力。它们学习到的稀疏表示能够通过有针对性的神经元干预实现成功的引导，并有助于详细分析跨检查点和层的神经元形成过程。

Conclusion: TopK LMs是理解语言模型如何学习和表示概念的稳定可靠工具，有望显著推进模型可解释性和可控性的未来研究。

Abstract: Sparse autoencoders (SAEs) have become an important tool for analyzing and
interpreting the activation space of transformer-based language models (LMs).
However, SAEs suffer several shortcomings that diminish their utility and
internal validity. Since SAEs are trained post-hoc, it is unclear if the
failure to discover a particular concept is a failure on the SAE's side or due
to the underlying LM not representing this concept. This problem is exacerbated
by training conditions and architecture choices affecting which features an SAE
learns. When tracing how LMs learn concepts during training, the lack of
feature stability also makes it difficult to compare SAEs features across
different checkpoints. To address these limitations, we introduce a
modification to the transformer architecture that incorporates a TopK
activation function at chosen layers, making the model's hidden states
equivalent to the latent features of a TopK SAE. This approach eliminates the
need for post-hoc training while providing interpretability comparable to SAEs.
The resulting TopK LMs offer a favorable trade-off between model size,
computational efficiency, and interpretability. Despite this simple
architectural change, TopK LMs maintain their original capabilities while
providing robust interpretability benefits. Our experiments demonstrate that
the sparse representations learned by TopK LMs enable successful steering
through targeted neuron interventions and facilitate detailed analysis of
neuron formation processes across checkpoints and layers. These features make
TopK LMs stable and reliable tools for understanding how language models learn
and represent concepts, which we believe will significantly advance future
research on model interpretability and controllability.

</details>


### [33] [Bridging Offline and Online Reinforcement Learning for LLMs](https://arxiv.org/abs/2506.21495)
*Jack Lanchantin,Angelica Chen,Janice Lan,Xian Li,Swarnadeep Saha,Tianlu Wang,Jing Xu,Ping Yu,Weizhe Yuan,Jason E Weston,Sainbayar Sukhbaatar,Ilia Kulikov*

Main category: cs.CL

TL;DR: 论文探究了RL方法在不同在线模式下微调LLM的效果，发现在线和半在线的DPO与GRPO表现相似且显著优于离线方法，多任务学习能进一步提升性能。


<details>
  <summary>Details</summary>
Motivation: 研究在从离线到半在线再到完全在线的各种场景下，强化学习方法在微调大型语言模型（LLM）方面的有效性，涵盖可验证和不可验证任务。

Method: 比较了在线和半在线的直接偏好优化（DPO）和群组奖励策略优化（GRPO）目标。实验包括可验证的数学任务和不可验证的指令遵循任务。详细分析了训练动态和超参数选择策略。展示了结合可验证和不可验证奖励的多任务学习。

Result: DPO和GRPO的在线和半在线变体表现出相似的性能和收敛性，并且都显著优于离线方法。结合可验证和不可验证奖励的多任务学习能够提高两种任务类型的性能。

Conclusion: 强化学习方法在不同在线模式和任务类型下微调大型语言模型非常有效，特别是DPO和GRPO的在线及半在线变体，它们显著优于离线方法。多任务学习结合不同奖励类型可以进一步提升整体性能。

Abstract: We investigate the effectiveness of reinforcement learning methods for
finetuning large language models when transitioning from offline to semi-online
to fully online regimes for both verifiable and non-verifiable tasks. Our
experiments cover training on verifiable math as well as non-verifiable
instruction following with a set of benchmark evaluations for both. Across
these settings, we extensively compare online and semi-online Direct Preference
Optimization and Group Reward Policy Optimization objectives, and surprisingly
find similar performance and convergence between these variants, which all
strongly outperform offline methods. We provide a detailed analysis of the
training dynamics and hyperparameter selection strategies to achieve optimal
results. Finally, we show that multi-tasking with verifiable and non-verifiable
rewards jointly yields improved performance across both task types.

</details>


### [34] [Enhancing User Engagement in Socially-Driven Dialogue through Interactive LLM Alignments](https://arxiv.org/abs/2506.21497)
*Jiashuo Wang,Kaitao Song,Chunpu Xu,Changhe Song,Yang Xiao,Dongsheng Li,Lili Qiu,Wenjie Li*

Main category: cs.CL

TL;DR: 通过用户模拟器和i×MCTS收集偏好数据，并利用未来对话信号（用户反应）通过DPO对LLM进行对齐，从而提升LLM的用户参与度。


<details>
  <summary>Details</summary>
Motivation: 以往的工作虽然优化模型来推理相关知识或规划对话行为流程，但用户参与度与知识或对话行为之间的关系微妙，不能保证在社交驱动对话中的用户参与度。为了解决这个问题，本文旨在使交互式LLM能够利用未来对话发展的信号来学习用户参与度。

Method: 1. 采用用户交互后与对话意图相关的用户反应作为奖励信号，作为用户参与度的直接指标。
2. 开发一个用户模拟器与目标交互式LLM进行交互。
3. 通过 i×MCTS（用于交互的蒙特卡洛树搜索）探索用户与交互式LLM系统之间的交互。
4. 收集包含更高和更低质量体验对的数据集。
5. 通过直接偏好优化（DPO）对齐交互式LLM，以实现高水平的用户参与度。

Result: 在两种社交驱动对话场景（情感支持对话和善意说服）中进行的实验表明，我们的方法有效提升了交互式LLM中的用户参与度。

Conclusion: 本研究提出的方法，通过利用未来对话信号、i×MCTS和用户模拟器与DPO相结合，有效提升了社交驱动对话中交互式LLM的用户参与度。

Abstract: Enhancing user engagement through interactions plays an essential role in
socially-driven dialogues. While prior works have optimized models to reason
over relevant knowledge or plan a dialogue act flow, the relationship between
user engagement and knowledge or dialogue acts is subtle and does not guarantee
user engagement in socially-driven dialogues. To this end, we enable
interactive LLMs to learn user engagement by leveraging signals from the future
development of conversations. Specifically, we adopt a more direct and relevant
indicator of user engagement, i.e., the user's reaction related to dialogue
intention after the interaction, as a reward to align interactive LLMs. To
achieve this, we develop a user simulator to interact with target interactive
LLMs and explore interactions between the user and the interactive LLM system
via \textit{i$\times$MCTS} (\textit{M}onte \textit{C}arlo \textit{T}ree
\textit{S}earch for \textit{i}nteraction). In this way, we collect a dataset
containing pairs of higher and lower-quality experiences using
\textit{i$\times$MCTS}, and align interactive LLMs for high-level user
engagement by direct preference optimization (DPO) accordingly. Experiments
conducted on two socially-driven dialogue scenarios (emotional support
conversations and persuasion for good) demonstrate that our method effectively
enhances user engagement in interactive LLMs.

</details>


### [35] [skLEP: A Slovak General Language Understanding Benchmark](https://arxiv.org/abs/2506.21508)
*Marek Šuppa,Andrej Ridzik,Daniel Hládek,Tomáš Javůrek,Viktória Ondrejová,Kristína Sásiková,Martin Tamajka,Marián Šimko*

Main category: cs.CL

TL;DR: 该论文介绍了skLEP，首个用于评估斯洛伐克语自然语言理解（NLU）模型的综合基准，包含九项任务。作者创建了新数据集并翻译了现有资源，首次对多种斯洛伐克语、多语言和英语预训练模型进行了系统评估。同时，论文还发布了完整的基准数据、开源工具包和公开排行榜，以促进斯洛伐克语NLU的未来研究。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏专门用于评估斯洛伐克语自然语言理解（NLU）模型的综合基准，因此需要创建一个涵盖多种任务的基准来全面评估模型能力。

Method: 1. 编译了skLEP，这是一个包含9项不同任务的综合基准，涵盖了词元级、句子对和文档级挑战。
2. 策划了为斯洛伐克语量身定制的全新原创数据集，并精心翻译了已有的英语NLU资源。
3. 使用skLEP任务，首次系统且广泛地评估了各种斯洛伐克语专用、多语言和英语预训练语言模型。

Result: 1. 成功创建了skLEP，这是首个用于评估斯洛伐克语NLU模型的综合基准。
2. 完成了对多种斯洛伐克语、多语言和英语预训练语言模型在skLEP任务上的首次系统性评估。
3. 发布了完整的基准数据、一个用于模型微调和评估的开源工具包，以及一个公开排行榜。

Conclusion: 通过发布skLEP基准数据、开源工具包和公共排行榜，旨在促进斯洛伐克语NLU研究的可复现性并推动未来的研究发展。

Abstract: In this work, we introduce skLEP, the first comprehensive benchmark
specifically designed for evaluating Slovak natural language understanding
(NLU) models. We have compiled skLEP to encompass nine diverse tasks that span
token-level, sentence-pair, and document-level challenges, thereby offering a
thorough assessment of model capabilities. To create this benchmark, we curated
new, original datasets tailored for Slovak and meticulously translated
established English NLU resources. Within this paper, we also present the first
systematic and extensive evaluation of a wide array of Slovak-specific,
multilingual, and English pre-trained language models using the skLEP tasks.
Finally, we also release the complete benchmark data, an open-source toolkit
facilitating both fine-tuning and evaluation of models, and a public
leaderboard at https://github.com/slovak-nlp/sklep in the hopes of fostering
reproducibility and drive future research in Slovak NLU.

</details>


### [36] [Potemkin Understanding in Large Language Models](https://arxiv.org/abs/2506.21521)
*Marina Mancoridis,Bec Weeks,Keyon Vafa,Sendhil Mullainathan*

Main category: cs.CL

TL;DR: 大型语言模型在人类基准测试上的表现可能只是一种“波将金式理解”（虚假理解），因为它们的误解方式与人类不同，且内部概念表示不连贯。文章提出了量化这种现象的方法，并发现其普遍存在。


<details>
  <summary>Details</summary>
Motivation: 论文旨在探讨使用基准数据集评估大型语言模型（LLMs）的合理性，特别是当这些基准同时用于测试人类时。它提出了一个问题：如果LLMs的误解方式不像人类那样，那么它们在基准上的成功可能只是一种“波将金式理解”（即表面理解的假象，但实际理解与人类解释概念的方式不符）。

Method: 论文首先引入了一个形式化框架来解决评估问题。接着，提出了两种量化“波将金式理解”的方法：一种是使用在三个领域专门设计的基准，另一种是提供其普遍性下限的通用程序。

Result: 研究发现，“波将金式理解”在模型、任务和领域中普遍存在。此外，这些失败不仅反映了不正确的理解，还反映了概念表示中更深层次的内部不连贯性。

Conclusion: LLMs在基于人类测试的基准上取得成功可能仅是“波将金式理解”，因为它们的内部概念表示可能不连贯，且它们的误解方式不一定与人类相同。因此，仅依赖此类基准来评估LLMs的真实能力是值得怀疑的。

Abstract: Large language models (LLMs) are regularly evaluated using benchmark
datasets. But what justifies making inferences about an LLM's capabilities
based on its answers to a curated set of questions? This paper first introduces
a formal framework to address this question. The key is to note that the
benchmarks used to test LLMs -- such as AP exams -- are also those used to test
people. However, this raises an implication: these benchmarks are only valid
tests if LLMs misunderstand concepts in ways that mirror human
misunderstandings. Otherwise, success on benchmarks only demonstrates potemkin
understanding: the illusion of understanding driven by answers irreconcilable
with how any human would interpret a concept. We present two procedures for
quantifying the existence of potemkins: one using a specially designed
benchmark in three domains, the other using a general procedure that provides a
lower-bound on their prevalence. We find that potemkins are ubiquitous across
models, tasks, and domains. We also find that these failures reflect not just
incorrect understanding, but deeper internal incoherence in concept
representations.

</details>


### [37] ["What's Up, Doc?": Analyzing How Users Seek Health Information in Large-Scale Conversational AI Datasets](https://arxiv.org/abs/2506.21532)
*Akshay Paruchuri,Maryam Aziz,Rohit Vartak,Ayman Ali,Best Uchehara,Xin Liu,Ishan Chatterjee,Monica Agrawal*

Main category: cs.CL

TL;DR: 该论文构建了HealthChat-11K数据集，并系统分析了用户使用大型语言模型获取医疗信息的行为模式和潜在风险，强调LLM在医疗保健支持方面仍需改进。


<details>
  <summary>Details</summary>
Motivation: 随着人们越来越多地通过交互式聊天机器人从大型语言模型（LLM）中获取医疗保健信息，这些对话的性质和固有风险仍未得到充分探索。

Method: 本文筛选了大规模对话式AI数据集，构建了HealthChat-11K，一个包含1.1万真实世界对话（2.5万用户消息）的精选数据集。研究人员利用HealthChat-11K和一个由临床医生驱动的用户与LLM交互分类法，系统地研究了21个不同健康专业的用户交互。

Result: 他们的分析揭示了用户如何以及为何寻求健康信息的洞察，例如常见交互、不完整上下文的情况、情感行为以及可能导致谄媚的交互（如引导性问题）。

Conclusion: 这强调了需要改进作为对话式AI部署的LLM在医疗保健支持方面的能力。

Abstract: People are increasingly seeking healthcare information from large language
models (LLMs) via interactive chatbots, yet the nature and inherent risks of
these conversations remain largely unexplored. In this paper, we filter
large-scale conversational AI datasets to achieve HealthChat-11K, a curated
dataset of 11K real-world conversations composed of 25K user messages. We use
HealthChat-11K and a clinician-driven taxonomy for how users interact with LLMs
when seeking healthcare information in order to systematically study user
interactions across 21 distinct health specialties. Our analysis reveals
insights into the nature of how and why users seek health information, such as
common interactions, instances of incomplete context, affective behaviors, and
interactions (e.g., leading questions) that can induce sycophancy, underscoring
the need for improvements in the healthcare support capabilities of LLMs
deployed as conversational AI. Code and artifacts to retrieve our analyses and
combine them into a curated dataset can be found here:
https://github.com/yahskapar/HealthChat

</details>


### [38] [Data Efficacy for Language Model Training](https://arxiv.org/abs/2506.21545)
*Yalun Dai,Yangyu Huang,Xin Zhang,Wenshan Wu,Chong Li,Wenhui Lu,Shijie Cao,Li Dong,Scarlett Li*

Main category: cs.CL

TL;DR: 这篇论文提出了DELT，一个用于优化语言模型训练数据组织的新范式。它引入了可学习性-质量评分（LQS）和折叠排序（FO），实验证明这些方法能在不增加数据量和模型大小的情况下显著提升模型性能，并强调了数据功效在LM训练中的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有研究多集中于数据效率（选择最小/最优训练数据子集），但数据功效（优化训练数据组织）相对未被充分探索。本文旨在通过优化数据组织来最大化语言模型性能。

Method: 本文提出了一个通用的范式DELT（Data Efficacy in LM Training），包含数据评分、数据选择和数据排序三个组件。具体地，提出了新的数据评分实例——可学习性-质量评分（LQS），以及新的数据排序实例——折叠排序（FO），以解决模型遗忘和数据分布偏差等问题。

Result: 综合实验验证了数据功效在LM训练中的有效性：1. 各种DELT实例能在不增加数据规模和模型大小的情况下，不同程度地提升LM性能。2. LQS结合FO实现了最显著的性能提升。3. 数据功效可以通过数据选择与数据效率协同实现。

Conclusion: 数据功效是语言模型训练中一个有前景的基础研究领域。

Abstract: Data is fundamental to the training of language models (LM). Recent research
has been dedicated to data efficiency, which aims to maximize performance by
selecting a minimal or optimal subset of training data. Techniques such as data
filtering, sampling, and selection play a crucial role in this area. To
complement it, we define Data Efficacy, which focuses on maximizing performance
by optimizing the organization of training data and remains relatively
underexplored. This work introduces a general paradigm, DELT, for considering
data efficacy in LM training, which highlights the significance of training
data organization. DELT comprises three components: Data Scoring, Data
Selection, and Data Ordering. Among these components, we design
Learnability-Quality Scoring (LQS), as a new instance of Data Scoring, which
considers both the learnability and quality of each data sample from the
gradient consistency perspective. We also devise Folding Ordering (FO), as a
novel instance of Data Ordering, which addresses issues such as model
forgetting and data distribution bias. Comprehensive experiments validate the
data efficacy in LM training, which demonstrates the following: Firstly,
various instances of the proposed DELT enhance LM performance to varying
degrees without increasing the data scale and model size. Secondly, among these
instances, the combination of our proposed LQS for data scoring and Folding for
data ordering achieves the most significant improvement. Lastly, data efficacy
can be achieved together with data efficiency by applying data selection.
Therefore, we believe that data efficacy is a promising foundational area in LM
training.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [39] [HumanOmniV2: From Understanding to Omni-Modal Reasoning with Context](https://arxiv.org/abs/2506.21277)
*Qize Yang,Shimin Yao,Weixuan Chen,Shenghao Fu,Detao Bai,Jiaxing Zhao,Boyuan Sun,Bowen Yin,Xihan Wei,Jingren Zhou*

Main category: cs.CV

TL;DR: 该论文通过强化学习和LLM评估的奖励（上下文、逻辑）来解决多模态LLM在理解人类意图推理中存在的上下文不足和捷径问题。他们的方法在多个全模态基准测试（包括新提出的IntentBench）上表现出先进性能。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（LLMs）需要深入理解和解释人类意图，这需要详细的推理能力。现有研究表明强化学习（RL）在增强LLMs推理能力方面有潜力，但其在多模态数据和格式上的应用仍面临挑战。当前多模态推理模型存在两个主要问题：全局上下文理解不足和捷径问题。

Method: 为解决现有问题，论文强调模型需清晰理解多模态输入中的全局上下文。通过LLM判断的上下文奖励、格式奖励和准确性奖励，确保多模态上下文的准确解释。此外，利用LLM评估逻辑奖励，确保推理过程逻辑地整合多模态信息。论文还引入了新的推理全模态基准IntentBench，用于评估模型理解复杂人类意图和情感的能力。

Result: 所提出的方法在多个全模态基准测试中，相较于其他开源全模态模型，展现出更优越的性能。

Conclusion: 该论文通过强调全局上下文理解和引入LLM评估的上下文及逻辑奖励，有效解决了多模态推理模型中存在的上下文理解不足和捷径问题，显著提升了模型理解复杂人类意图的能力。

Abstract: With the rapid evolution of multimodal large language models, the capacity to
deeply understand and interpret human intentions has emerged as a critical
capability, which demands detailed and thoughtful reasoning. In recent studies,
Reinforcement Learning (RL) has demonstrated potential in enhancing the
reasoning capabilities of Large Language Models (LLMs). Nonetheless, the
challenges associated with adapting RL to multimodal data and formats remain
largely unaddressed. In this paper, we identify two issues in existing
multimodal reasoning models: insufficient global context understanding and
shortcut problems. Insufficient context understanding can happen when a model
misinterprets multimodal context, resulting in incorrect answers. The shortcut
problem occurs when the model overlooks crucial clues in multimodal inputs,
directly addressing the query without considering the multimodal information.
To tackle these issues, we emphasize the necessity for the model to reason with
a clear understanding of the global context within multimodal inputs. This
global context understanding can effectively prevent the model from overlooking
key multimodal cues and ensure a thorough reasoning process. To ensure the
accurate interpretation of multimodal context information, we implement a
context reward judged by a large language model, alongside format and accuracy
rewards. Additionally, to improve complex reasoning capability, we employ the
LLM to assess the logical reward, determining whether the reasoning process
successfully integrates multimodal information with logical methods. We also
introduce a reasoning omni-modal benchmark, IntentBench, aimed at evaluating
models in understanding complex human intentions and emotions. Our proposed
method demonstrates advanced performance across multiple omni-modal benchmarks
compared to other open-source omni-modal models.

</details>


### [40] [Logios : An open source Greek Polytonic Optical Character Recognition system](https://arxiv.org/abs/2506.21474)
*Perifanos Konstantinos,Goutsos Dionisis*

Main category: cs.CV

TL;DR: 论文提出了一个用于识别希腊多调符号文本的OCR系统，通过结合卷积层和循环层，显著提高了准确性和效率，并发布了开源模型和平台。


<details>
  <summary>Details</summary>
Motivation: 解决希腊多调符号文本的独特挑战，克服传统OCR方法的局限性。

Method: 结合卷积层（用于特征提取）和循环层（用于序列学习）。

Result: 显著提高准确性和效率；发布开源模型作为开源库，并使OCR平台可供学术使用。

Conclusion: 开发了一个专门用于准确识别和数字化希腊多调符号文本的OCR系统。

Abstract: In this paper, we present an Optical Character Recognition (OCR) system
specifically designed for the accurate recognition and digitization of Greek
polytonic texts. By leveraging the combined strengths of convolutional layers
for feature extraction and recurrent layers for sequence learning, our system
addresses the unique challenges posed by Greek polytonic scripts. This approach
aims to overcome the limitations of traditional OCR methods, offering
significant improvements in accuracy and efficiency. We release the underlying
model as an open-source library and make our OCR platform available for
academic use.

</details>


### [41] [HalluSegBench: Counterfactual Visual Reasoning for Segmentation Hallucination Evaluation](https://arxiv.org/abs/2506.21546)
*Xinzhuo Li,Adheesh Juvekar,Xingyou Liu,Muntasir Wahed,Kiet A. Nguyen,Ismini Lourentzou*

Main category: cs.CV

TL;DR: 该论文介绍了HalluSegBench，这是一个用于评估视觉语言分割模型中幻觉的新基准，特别关注反事实视觉推理。实验表明，视觉驱动的幻觉比标签驱动的幻觉更普遍。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言分割模型经常出现幻觉，即生成与图像内容不符的分割掩码或错误标记不相关区域。当前的评估协议主要关注标签或文本幻觉，而忽略了视觉上下文操作，限制了其诊断关键故障的能力。

Method: 引入了HalluSegBench，这是第一个专门通过反事实视觉推理来评估视觉基础中幻觉的基准。该基准包括一个包含1340对反事实实例的新数据集（涵盖281个独特的对象类别），以及一套新引入的度量标准，用于量化视觉上连贯的场景编辑下的幻觉敏感性。

Result: 在HalluSegBench上对最先进的视觉语言分割模型进行的实验表明，视觉驱动的幻觉比标签驱动的幻觉更为普遍，模型通常会持续错误的分割。

Conclusion: 强调了反事实推理对于诊断接地保真度的必要性。

Abstract: Recent progress in vision-language segmentation has significantly advanced
grounded visual understanding. However, these models often exhibit
hallucinations by producing segmentation masks for objects not grounded in the
image content or by incorrectly labeling irrelevant regions. Existing
evaluation protocols for segmentation hallucination primarily focus on label or
textual hallucinations without manipulating the visual context, limiting their
capacity to diagnose critical failures. In response, we introduce
HalluSegBench, the first benchmark specifically designed to evaluate
hallucinations in visual grounding through the lens of counterfactual visual
reasoning. Our benchmark consists of a novel dataset of 1340 counterfactual
instance pairs spanning 281 unique object classes, and a set of newly
introduced metrics that quantify hallucination sensitivity under visually
coherent scene edits. Experiments on HalluSegBench with state-of-the-art
vision-language segmentation models reveal that vision-driven hallucinations
are significantly more prevalent than label-driven ones, with models often
persisting in false segmentation, highlighting the need for counterfactual
reasoning to diagnose grounding fidelity.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [42] [Exploring Adapter Design Tradeoffs for Low Resource Music Generation](https://arxiv.org/abs/2506.21298)
*Atharva Mehta,Shivam Chauhan,Monojit Choudhury*

Main category: cs.SD

TL;DR: 本文研究了在低资源音乐流派上微调大型音乐生成模型（如MusicGen和Mustango）时，不同适配器配置的优化问题。研究发现，卷积适配器擅长捕捉局部细节，而Transformer适配器则能保持长程依赖性。中等大小的适配器（40M参数）在表现力和质量之间达到了最佳平衡。此外，研究还比较了扩散模型（Mustango）和自回归模型（MusicGen）的性能和效率。


<details>
  <summary>Details</summary>
Motivation: 微调大型音乐生成模型计算成本高昂，而PEFT技术（特别是基于适配器的方法）虽然有前景，但其最佳设计选择（架构、放置、大小）尚不明确，尤其是在低资源音乐流派中。

Method: 通过研究MusicGen和Mustango两种AI音乐模型在印度斯坦古典音乐和土耳其马卡姆音乐这两种流派上的各种适配器配置来回答这一问题。

Result: 研究发现：
1. 卷积适配器擅长捕捉精细的局部音乐细节（如装饰音和短旋律片段）。
2. Transformer适配器能更好地保持对结构化即兴创作至关重要的长程依赖性。
3. 中等大小的适配器（40M参数）在表现力和质量之间达到了最佳平衡。
4. 基于扩散的模型Mustango生成更多样化的输出，更符合输入提示描述，但在音符稳定性、节奏对齐和美学方面有所欠缺，且计算密集，训练时间长。
5. 自回归模型MusicGen训练更快，效率更高，能产生更高质量的输出，但在生成中冗余略高。

Conclusion: 本研究揭示了不同适配器设计选择在音乐生成模型微调中的权衡，为低资源音乐流派的适配器优化提供了见解，并比较了扩散模型和自回归模型在PEFT背景下的性能和效率。

Abstract: Fine-tuning large-scale music generation models, such as MusicGen and
Mustango, is a computationally expensive process, often requiring updates to
billions of parameters and, therefore, significant hardware resources.
Parameter-Efficient Fine-Tuning (PEFT) techniques, particularly adapter-based
methods, have emerged as a promising alternative, enabling adaptation with
minimal trainable parameters while preserving model performance. However, the
design choices for adapters, including their architecture, placement, and size,
are numerous, and it is unclear which of these combinations would produce
optimal adapters and why, for a given case of low-resource music genre. In this
paper, we attempt to answer this question by studying various adapter
configurations for two AI music models, MusicGen and Mustango, on two genres:
Hindustani Classical and Turkish Makam music.
  Our findings reveal distinct trade-offs: convolution-based adapters excel in
capturing fine-grained local musical details such as ornamentations and short
melodic phrases, while transformer-based adapters better preserve long-range
dependencies crucial for structured improvisation. Additionally, we analyze
computational resource requirements across different adapter scales,
demonstrating how mid-sized adapters (40M parameters) achieve an optimal
balance between expressivity and quality. Furthermore, we find that Mustango, a
diffusion-based model, generates more diverse outputs with better adherence to
the description in the input prompt while lacking in providing stability in
notes, rhythm alignment, and aesthetics. Also, it is computationally intensive
and requires significantly more time to train. In contrast, autoregressive
models like MusicGen offer faster training and are more efficient, and can
produce better quality output in comparison, but have slightly higher
redundancy in their generations.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [43] [Leaner Training, Lower Leakage: Revisiting Memorization in LLM Fine-Tuning with LoRA](https://arxiv.org/abs/2506.20856)
*Fei Wang,Baochun Li*

Main category: cs.LG

TL;DR: LoRA微调可以显著降低大型语言模型的记忆风险，同时保持性能，并且其记忆模式与预训练和全量微调不同。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型中的记忆化使其容易受到数据提取攻击。预训练中的记忆化已被广泛研究，但微调（特别是LoRA微调）中的记忆化影响探索较少。

Method: 本文重新审视了微调中的记忆化，并比较了不同微调策略，特别关注LoRA微调与全量微调，使用了一种更宽松的基于相似性的记忆化度量。

Result: 与现有发现不同，模型规模和数据重复等影响预训练和全量微调记忆化的因素在LoRA微调中不遵循相同趋势。LoRA相比全量微调显著降低了记忆风险，同时保持了强大的任务性能。

Conclusion: LoRA微调能有效降低大型语言模型的记忆风险，同时维持良好的任务表现。

Abstract: Memorization in large language models (LLMs) makes them vulnerable to data
extraction attacks. While pre-training memorization has been extensively
studied, fewer works have explored its impact in fine-tuning, particularly for
LoRA fine-tuning, a widely adopted parameter-efficient method.
  In this work, we re-examine memorization in fine-tuning and uncover a
surprising divergence from prior findings across different fine-tuning
strategies. Factors such as model scale and data duplication, which strongly
influence memorization in pre-training and full fine-tuning, do not follow the
same trend in LoRA fine-tuning. Using a more relaxed similarity-based
memorization metric, we demonstrate that LoRA significantly reduces
memorization risks compared to full fine-tuning, while still maintaining strong
task performance.

</details>


### [44] [SharpZO: Hybrid Sharpness-Aware Vision Language Model Prompt Tuning via Forward-Only Passes](https://arxiv.org/abs/2506.20990)
*Yifan Yang,Zhen Zhang,Rupak Vignesh Swaminathan,Jing Liu,Nathan Susanj,Zheng Zhang*

Main category: cs.LG

TL;DR: SharpZO是一种无需反向传播的视觉语言模型微调方法，通过两阶段优化（锐度感知ES和稀疏零阶优化）提高准确性和收敛速度，适用于内存受限的边缘设备。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型微调需要反向传播，不适用于内存受限的边缘设备，且现有的无反向传播方法性能不佳。

Method: 提出混合锐度感知零阶优化（SharpZO），分两阶段：首先是锐度感知演化策略（ES）进行全局探索并平滑损失面以构建强初始化；其次是稀疏零阶优化进行精细局部搜索。整个优化过程仅依赖前向传播。

Result: 在CLIP模型上的实验表明，SharpZO比现有最先进的仅前向传播方法平均提高高达7%的性能。

Conclusion: SharpZO显著提高了零阶VLM微调的准确性和收敛速度。

Abstract: Fine-tuning vision language models (VLMs) has achieved remarkable performance
across various downstream tasks; yet, it requires access to model gradients
through backpropagation (BP), making them unsuitable for memory-constrained,
inference-only edge devices. To address this limitation, previous work has
explored various BP-free fine-tuning methods. However, these approaches often
rely on high-variance evolutionary strategies (ES) or zeroth-order (ZO)
optimization, and often fail to achieve satisfactory performance. In this
paper, we propose a hybrid Sharpness-aware Zeroth-order optimization (SharpZO)
approach, specifically designed to enhance the performance of ZO VLM
fine-tuning via a sharpness-aware warm-up training. SharpZO features a
two-stage optimization process: a sharpness-aware ES stage that globally
explores and smooths the loss landscape to construct a strong initialization,
followed by a fine-grained local search via sparse ZO optimization. The entire
optimization relies solely on forward passes. Detailed theoretical analysis and
extensive experiments on CLIP models demonstrate that SharpZO significantly
improves accuracy and convergence speed, achieving up to 7% average gain over
state-of-the-art forward-only methods.

</details>


### [45] [Enhancing LLM Tool Use with High-quality Instruction Data from Knowledge Graph](https://arxiv.org/abs/2506.21071)
*Jingwei Wang,Zai Zhang,Hao Qian,Chunjing Gan,Binbin Hu,Ziqi Liu,Zhiqiang Zhang,Jun Zhou,Bin Shi,Bo Dong*

Main category: cs.LG

TL;DR: 该论文提出了一种利用知识图谱为大型语言模型（LLMs）生成高质量指令数据的新方法，只需少量微调数据即可显著提升LLMs的工具使用能力和整体性能。


<details>
  <summary>Details</summary>
Motivation: 教授大型语言模型（LLMs）使用工具对于提高其问题解决能力和扩展应用至关重要，但现有方法生成的指令数据质量不足。

Method: 通过知识图谱生成高质量指令数据，具体步骤包括：从知识图谱中提取查询路径并转化为用户查询；将实体间关系转化为可操作的工具；将查询路径解析为详细的解决方案步骤。

Result: 在少量合成数据上进行微调即可显著提高LLMs的工具利用率和整体能力。

Conclusion: 利用知识图谱可以有效生成高质量指令数据，从而显著提升大型语言模型的工具使用能力。

Abstract: Teaching large language models (LLMs) to use tools is crucial for improving
their problem-solving abilities and expanding their applications. However,
effectively using tools is challenging because it requires a deep understanding
of tool functionalities and user intentions. Previous methods relied mainly on
LLMs to generate instruction data, but the quality of these data was often
insufficient. In this paper, we propose a new method that uses knowledge graphs
to generate high-quality instruction data for LLMs. Knowledge graphs are
manually curated datasets rich in semantic information. We begin by extracting
various query pathways from a given knowledge graph, which are transformed into
a broad spectrum of user queries. We then translate the relationships between
entities into actionable tools and parse the pathways of each query into
detailed solution steps, thereby creating high-quality instruction data. Our
experiments show that fine-tuning on just a small sample of this synthetic data
can significantly improve the tool utilization and overall capabilities of
LLMs.

</details>


### [46] [Learning to Skip the Middle Layers of Transformers](https://arxiv.org/abs/2506.21103)
*Tim Lawson,Laurence Aitchison*

Main category: cs.LG

TL;DR: 该论文提出了一种新的Transformer架构，通过动态跳过中间层来提高效率，但实验结果显示与基线模型相比，该方法在验证交叉熵和FLOPs之间的权衡上没有改进。


<details>
  <summary>Details</summary>
Motivation: 现有的Transformer条件计算方法通常针对独立模块或跳过层，但可解释性研究表明Transformer的中间层存在冗余，早期层聚合信息。受此启发，本文旨在通过动态跳过中间层来提高Transformer的效率。

Method: 本文提出了一种新的Transformer架构，该架构根据输入动态跳过可变数量的中间层。具体而言，一个学习到的门控机制决定是否绕过对称的中心块范围，一个门控注意力机制阻止后续token关注被跳过的token位置。残差范数通过“三明治”或“perilayernorm”方案控制，门控稀疏性通过自适应正则化损失控制。

Result: 在所研究的规模下，与层数较少的密集基线模型相比，该方法在验证交叉熵和估计FLOPs之间的权衡方面没有实现改进。

Conclusion: 尽管旨在减少计算需求并促进多级表示层次，但所提出的动态跳过中间层的方法未能提升Transformer在效率与性能之间的权衡。

Abstract: Conditional computation is a popular strategy to make Transformers more
efficient. Existing methods often target individual modules (e.g.,
mixture-of-experts layers) or skip layers independently of one another.
However, interpretability research has demonstrated that the middle layers of
Transformers exhibit greater redundancy, and that early layers aggregate
information into token positions. Guided by these insights, we propose a novel
architecture that dynamically skips a variable number of layers from the middle
outward. In particular, a learned gating mechanism determines whether to bypass
a symmetric span of central blocks based on the input, and a gated attention
mechanism prevents subsequent tokens from attending to skipped token positions.
Residual norms are controlled with a 'sandwich' or 'perilayernorm' scheme and
gate sparsity with an adaptive regularization loss. We had aimed to reduce
compute requirements for 'simpler' tokens and potentially foster an emergent
multi-level representational hierarchy but, at the scales investigated, our
approach does not achieve improvements in the trade-off between validation
cross-entropy and estimated FLOPs compared to dense baselines with fewer
layers. We release our code at https://github.com/tim-lawson/skip-middle.

</details>


### [47] [Complexity-aware fine-tuning](https://arxiv.org/abs/2506.21220)
*Andrey Goncharov,Daniil Vyazhev,Petr Sychev,Edvard Khalafyan,Alexey Zaytsev*

Main category: cs.LG

TL;DR: 提出一种基于熵识别复杂数据的高效LLM微调方法，性能优于SFT，且以更少数据达到蒸馏效果。


<details>
  <summary>Details</summary>
Motivation: 针对特定领域，LLM通常通过SFT进行微调。虽然大模型的思维链蒸馏能获得更好的结果，但成本高昂且需要大量数据。

Method: 提出了一种新颖的高效微调方案，仅对通过熵识别的复杂数据使用推理。具体做法是：将训练数据根据单token答案熵（ROC AUC 0.73）分为不同复杂类别，并通过SFT和蒸馏对LLMs进行微调。

Result: 该方案在两个小型开放模型上显著优于标准SFT方法（平均准确率0.55 vs 0.43），并且在数据量减少62%的情况下，性能与蒸馏方法相当（两者平均准确率均为0.55）。

Conclusion: 论文提出了一种高效的LLM微调方法，该方法通过智能地利用复杂数据上的推理，实现了比传统SFT更好的性能，并显著减少了蒸馏所需的额外数据，为LLM微调提供了新的范式。

Abstract: General-purpose Large Language Models (LLMs) are frequently fine-tuned
through supervised fine-tuning (SFT) to enhance performance in specific
domains. Better results can be achieved by distilling the chain-of-thought of a
larger model at the cost of numerous expensive calls and a much greater amount
of data. We propose a novel blueprint for efficient fine-tuning that uses
reasoning only for complex data identified by entropy. Specifically, across two
small open models ($\approx 3B$) we split the training data into complexity
categories by a single token answer entropy (ROC AUC $0.73$), fine-tune large
language models (LLMs) via SFT and distillation, and show that our pipeline
significantly outperforms the standard SFT approach ($0.55$ vs $0.43$ average
accuracy) and provides comparable with distillation performance while using
$62\%$ less data ($0.55$ average accuracy for both). We publish our code and
data to facilitate further research in this direction.

</details>


### [48] [DiLoCoX: A Low-Communication Large-Scale Training Framework for Decentralized Cluster](https://arxiv.org/abs/2506.21263)
*Ji Qi,WenPeng Zhu,Li Li,Ming Wu,YingJun Wu,Wu He,Xun Gao,Jason Zeng,Michael Heinrich*

Main category: cs.LG

TL;DR: DiLoCoX是一个低通信量的去中心化训练框架，能够使千亿级大模型在慢速网络上高效训练，显著提升速度并保持收敛性。


<details>
  <summary>Details</summary>
Motivation: 分布式训练大语言模型需要高通信量，依赖中心化集群和快速互联。研究旨在解决如何在慢速网络上对超千亿参数模型进行去中心化训练。

Method: 提出了DiLoCoX框架，结合了管道并行、双优化器策略、通信与本地训练的一步延迟重叠以及自适应梯度压缩方案。

Result: DiLoCoX能在1Gbps网络上预训练107B基础模型，相比AllReduce，分布式训练速度提升357倍，模型收敛性退化可忽略不计。

Conclusion: DiLoCoX是首个成功应用于千亿级参数模型的去中心化训练框架。

Abstract: The distributed training of foundation models, particularly large language
models (LLMs), demands a high level of communication. Consequently, it is
highly dependent on a centralized cluster with fast and reliable interconnects.
Can we conduct training on slow networks and thereby unleash the power of
decentralized clusters when dealing with models exceeding 100 billion
parameters? In this paper, we propose DiLoCoX, a low-communication large-scale
decentralized cluster training framework. It combines Pipeline Parallelism with
Dual Optimizer Policy, One-Step-Delay Overlap of Communication and Local
Training, and an Adaptive Gradient Compression Scheme. This combination
significantly improves the scale of parameters and the speed of model
pre-training. We justify the benefits of one-step-delay overlap of
communication and local training, as well as the adaptive gradient compression
scheme, through a theoretical analysis of convergence. Empirically, we
demonstrate that DiLoCoX is capable of pre-training a 107B foundation model
over a 1Gbps network. Compared to vanilla AllReduce, DiLoCoX can achieve a 357x
speedup in distributed training while maintaining negligible degradation in
model convergence. To the best of our knowledge, this is the first
decentralized training framework successfully applied to models with over 100
billion parameters.

</details>


### [49] [Latent Prototype Routing: Achieving Near-Perfect Load Balancing in Mixture-of-Experts](https://arxiv.org/abs/2506.21328)
*Jiajie Yang*

Main category: cs.LG

TL;DR: 提出LPR，一种新的专家路由方法，解决了MoE模型中的负载不平衡问题，显著提高了专家利用率。


<details>
  <summary>Details</summary>
Motivation: 当前MoE系统存在严重的负载不平衡问题，导致模型容量和计算资源利用率低下。

Method: 本文从聚类角度重新审视专家路由，并提出了潜在原型路由（LPR）框架，该框架泛化了现有方法，并在不损害下游性能的情况下促进了专家负载的平衡利用。

Result: LPR在多个开源MoE模型（DeepSeek-V3、Qwen3-MoE和Mixtral）上将专家负载的基尼系数从0.70平均降低到0.035，将最小-最大专家负载比从1e-6提高到0.70，实现了接近完美的负载平衡。

Conclusion: LPR有效地解决了MoE架构中的负载不平衡问题，实现了接近完美的负载均衡，且不影响性能。

Abstract: Mixture-of-Experts (MoE) architectures have emerged as a key strategy for
scaling large language models (LLMs) efficiently. However, current MoE systems
suffer from severe load imbalance, where only a small subset of experts is
consistently activated during training and inference, leading to significant
underutilization of model capacity and computational resources. In this work,
we revisit expert routing through a clustering perspective and propose Latent
Prototype Routing (LPR), a novel routing framework that generalizes existing
approaches while promoting balanced expert utilization without compromising
downstream performance. Extensive experiments across multiple open-source MoE
models -- including DeepSeek-V3, Qwen3-MoE, and Mixtral -- demonstrate that LPR
reduces the Gini coefficient of expert load from 0.70 to 0.035 on average,
improves the min-max expert load ratio from 1e-6 to 0.70, achieving
near-perfect load balancing.

</details>


### [50] [Scalable Bayesian Low-Rank Adaptation of Large Language Models via Stochastic Variational Subspace Inference](https://arxiv.org/abs/2506.21408)
*Colin Samplawski,Adam D. Cobb,Manoj Acharya,Ramneet Kaur,Susmit Jha*

Main category: cs.LG

TL;DR: ScalaBL通过在低维子空间中进行贝叶斯推断，为大型语言模型提供了可扩展的不确定性量化方法，显著减少了额外参数，并能应用于更大的模型。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）存在生成不正确信息和校准不良的问题，这使得LLMs的不确定性量化变得至关重要，尤其是在高风险领域（如自动驾驶和医疗保健）。现有的基于贝叶斯深度学习的方法难以扩展到更大的LLMs，因为它们需要额外的参数，且比LoRA更多。

Method: 本文提出了ScalaBL（Scalable Bayesian Low-Rank Adaptation via Stochastic Variational Subspace Inference）。该方法在LoRA秩r的r维子空间中进行贝叶斯推断。通过将LoRA参数重新用作投影矩阵，能够将子空间中的样本映射到LLM的完整权重空间。所有参数都通过随机变分推断学习。

Result: 尽管子空间维度较低，但ScalaBL能够与最先进的方法达到竞争性能，且仅需要约1000个额外的参数。此外，它能够扩展到迄今为止最大的贝叶斯LLM，其基础参数是现有工作的四倍。

Conclusion: ScalaBL为大型语言模型提供了一种有效且可扩展的不确定性量化方法。通过在低维子空间中执行贝叶斯推断，它显著减少了所需的额外参数，从而能够应用于更大的模型并实现与现有技术相当的性能。

Abstract: Despite their widespread use, large language models (LLMs) are known to
hallucinate incorrect information and be poorly calibrated. This makes the
uncertainty quantification of these models of critical importance, especially
in high-stakes domains, such as autonomy and healthcare. Prior work has made
Bayesian deep learning-based approaches to this problem more tractable by
performing inference over the low-rank adaptation (LoRA) parameters of a
fine-tuned model. While effective, these approaches struggle to scale to larger
LLMs due to requiring further additional parameters compared to LoRA. In this
work we present $\textbf{Scala}$ble $\textbf{B}$ayesian $\textbf{L}$ow-Rank
Adaptation via Stochastic Variational Subspace Inference (ScalaBL). We perform
Bayesian inference in an $r$-dimensional subspace, for LoRA rank $r$. By
repurposing the LoRA parameters as projection matrices, we are able to map
samples from this subspace into the full weight space of the LLM. This allows
us to learn all the parameters of our approach using stochastic variational
inference. Despite the low dimensionality of our subspace, we are able to
achieve competitive performance with state-of-the-art approaches while only
requiring ${\sim}1000$ additional parameters. Furthermore, it allows us to
scale up to the largest Bayesian LLM to date, with four times as a many base
parameters as prior work.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [51] [MAGPIE: A dataset for Multi-AGent contextual PrIvacy Evaluation](https://arxiv.org/abs/2506.20737)
*Gurusha Juneja,Alon Albalak,Wenyue Hua,William Yang Wang*

Main category: cs.AI

TL;DR: LLM代理在多代理协作中缺乏上下文隐私理解和保护能力，即使有明确指示也常泄露隐私，导致任务失败。新基准MAGPIE揭示了现有SOTA模型在此方面的不足。


<details>
  <summary>Details</summary>
Motivation: 探讨基于LLM的智能体在多智能体协作中是否理解并能维护上下文隐私，因为这类系统常涉及敏感信息，隐私保护至关重要。

Method: 提出并构建了一个名为MAGPIE的基准，包含15个领域158个高风险真实场景。在此基准上评估了GPT-4o和Claude-2.7-Sonnet等现有最先进的LLM模型，以测试它们对上下文隐私的理解能力以及在协作中不侵犯用户隐私的能力。

Result: 当前模型（如GPT-4o和Claude-2.7-Sonnet）缺乏对上下文隐私的鲁棒理解，分别有25.2%和43.6%的时间错误地将隐私数据分类为可共享数据。在多轮对话中，即使有明确的隐私指令，这些模型仍分别在59.9%和50.5%的案例中泄露隐私信息。多智能体系统在71%的场景中未能完成任务。

Conclusion: 目前的大语言模型在上下文隐私保护和协作任务解决方面未能实现良好对齐。

Abstract: The proliferation of LLM-based agents has led to increasing deployment of
inter-agent collaboration for tasks like scheduling, negotiation, resource
allocation etc. In such systems, privacy is critical, as agents often access
proprietary tools and domain-specific databases requiring strict
confidentiality. This paper examines whether LLM-based agents demonstrate an
understanding of contextual privacy. And, if instructed, do these systems
preserve inference time user privacy in non-adversarial multi-turn
conversation. Existing benchmarks to evaluate contextual privacy in LLM-agents
primarily assess single-turn, low-complexity tasks where private information
can be easily excluded. We first present a benchmark - MAGPIE comprising 158
real-life high-stakes scenarios across 15 domains. These scenarios are designed
such that complete exclusion of private data impedes task completion yet
unrestricted information sharing could lead to substantial losses. We then
evaluate the current state-of-the-art LLMs on (a) their understanding of
contextually private data and (b) their ability to collaborate without
violating user privacy. Empirical experiments demonstrate that current models,
including GPT-4o and Claude-2.7-Sonnet, lack robust understanding of contextual
privacy, misclassifying private data as shareable 25.2\% and 43.6\% of the
time. In multi-turn conversations, these models disclose private information in
59.9\% and 50.5\% of cases even under explicit privacy instructions.
Furthermore, multi-agent systems fail to complete tasks in 71\% of scenarios.
These results underscore that current models are not aligned towards both
contextual privacy preservation and collaborative task-solving.

</details>


### [52] [Beyond Reactive Safety: Risk-Aware LLM Alignment via Long-Horizon Simulation](https://arxiv.org/abs/2506.20949)
*Chenkai Sun,Denghui Zhang,ChengXiang Zhai,Heng Ji*

Main category: cs.AI

TL;DR: 为了提高语言模型代理的安全性，本文提出了一个框架来预测模型建议的社会传播，并引入了一个间接危害数据集。实验结果在新数据集上性能提升超过20%，并在现有安全基准上胜率超过70%，表明了实现更安全代理的潜力。


<details>
  <summary>Details</summary>
Motivation: 鉴于语言模型代理对高风险社会决策（如公共政策和医疗保健）日益增长的影响，确保其有益影响需要理解其建议的深远影响。

Method: 提出了一个概念验证框架，可以预测模型生成的建议如何随时间在宏观层面上传播。同时引入了一个包含100个间接危害情景的数据集，用于评估模型预见看似无害的用户提示可能导致的意想不到的有害结果的能力。

Result: 在新数据集上实现了超过20%的改进，并且在现有安全基准（AdvBench、SafeRLHF、WildGuardMix）上对强基线取得了超过70%的平均胜率。

Conclusion: 建议了一个使代理更安全的有前景的方向。

Abstract: Given the growing influence of language model-based agents on high-stakes
societal decisions, from public policy to healthcare, ensuring their beneficial
impact requires understanding the far-reaching implications of their
suggestions. We propose a proof-of-concept framework that projects how
model-generated advice could propagate through societal systems on a
macroscopic scale over time, enabling more robust alignment. To assess the
long-term safety awareness of language models, we also introduce a dataset of
100 indirect harm scenarios, testing models' ability to foresee adverse,
non-obvious outcomes from seemingly harmless user prompts. Our approach
achieves not only over 20% improvement on the new dataset but also an average
win rate exceeding 70% against strong baselines on existing safety benchmarks
(AdvBench, SafeRLHF, WildGuardMix), suggesting a promising direction for safer
agents.

</details>


### [53] [Unveiling Causal Reasoning in Large Language Models: Reality or Mirage?](https://arxiv.org/abs/2506.21215)
*Haoang Chi,He Li,Wenjing Yang,Feng Liu,Long Lan,Xiaoguang Ren,Tongliang Liu,Bo Han*

Main category: cs.AI

TL;DR: 该论文探讨了大型语言模型（LLMs）是否具备真正的人类式因果推理能力。研究发现LLMs目前仅限于浅层（level-1）因果推理，主要依赖参数中嵌入的因果知识。通过引入新的基准CausalProbe-2024，证实了LLMs在此方面的不足。为弥补这一差距，论文提出了G^2-Reasoner方法，通过融入通用知识和目标导向提示，显著提升了LLMs在因果推理上的表现，为LLMs向更深层次的因果推理迈进提供了新途径。


<details>
  <summary>Details</summary>
Motivation: LLMs在理解因果关系方面表现出了一定能力，但仍不清楚它们是否能进行真正的人类式因果推理。现有证据表明，LLMs主要进行浅层（level-1）因果推理，缺乏深层（level-2）的人类式因果推理能力。

Method: 1. 方法论层面：深入分析基于Transformer的LLMs的自回归机制，揭示其并非天生具有因果性。
2. 经验层面：引入新的因果问答基准CausalProbe-2024，该基准的语料库对LLMs来说是新鲜且几乎未见的。
3. 提出G^2-Reasoner：借鉴人类推理受通用知识和目标驱动的启发，将通用知识和目标导向提示融入LLMs的因果推理过程。

Result: 1. LLMs在CausalProbe-2024上的表现显著下降，表明它们主要进行level-1因果推理。
2. G^2-Reasoner显著增强了LLMs的因果推理能力，尤其是在新鲜和反事实的上下文中。

Conclusion: LLMs目前仅限于浅层（level-1）因果推理。G^2-Reasoner方法通过整合通用知识和目标导向提示，为LLMs实现更深层次（level-2）的真正因果推理提供了新的方向。

Abstract: Causal reasoning capability is critical in advancing large language models
(LLMs) toward strong artificial intelligence. While versatile LLMs appear to
have demonstrated capabilities in understanding contextual causality and
providing responses that obey the laws of causality, it remains unclear whether
they perform genuine causal reasoning akin to humans. However, current evidence
indicates the contrary. Specifically, LLMs are only capable of performing
shallow (level-1) causal reasoning, primarily attributed to the causal
knowledge embedded in their parameters, but they lack the capacity for genuine
human-like (level-2) causal reasoning. To support this hypothesis,
methodologically, we delve into the autoregression mechanism of
transformer-based LLMs, revealing that it is not inherently causal.
Empirically, we introduce a new causal Q&A benchmark called CausalProbe-2024,
whose corpora are fresh and nearly unseen for the studied LLMs. The LLMs
exhibit a significant performance drop on CausalProbe-2024 compared to earlier
benchmarks, indicating the fact that they primarily engage in level-1 causal
reasoning. To bridge the gap towards level-2 causal reasoning, we draw
inspiration from the fact that human reasoning is usually facilitated by
general knowledge and intended goals. We propose G^2-Reasoner, a method that
incorporates general knowledge and goal-oriented prompts into LLMs' causal
reasoning processes. Experiments demonstrate that G^2-Reasoner significantly
enhances LLMs' causal reasoning capability, particularly in fresh and
counterfactual contexts. This work sheds light on a new path for LLMs to
advance towards genuine causal reasoning, going beyond level-1 and making
strides towards level-2.

</details>


### [54] [Spatial Mental Modeling from Limited Views](https://arxiv.org/abs/2506.21458)
*Baiqiao Yin,Qineng Wang,Pingyue Zhang,Jianshu Zhang,Kangrui Wang,Zihan Wang,Jieyu Zhang,Keshigeyan Chandrasegaran,Han Liu,Ranjay Krishna,Saining Xie,Manling Li,Jiajun Wu,Li Fei-Fei*

Main category: cs.AI

TL;DR: 视觉语言模型（VLMs）在从少量视角想象完整场景方面存在不足，无法像人类那样构建空间心理模型。本文提出了MindCube基准来评估这一差距，并引入了一种“先地图后推理”的方法，结合认知地图和强化学习，显著提高了VLM对不可观测空间的理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型（VLMs）无法像人类一样，从少量视角想象出完整的场景并形成空间心理模型。这导致了它们在理解布局、透视和运动方面的关键差距。本文旨在揭示并解决这一问题，通过引入MindCube基准来暴露现有VLM在此方面的近乎随机的性能。

Method: 本文提出了MindCube基准测试（包含3,268张图片中的21,154个问题），以系统评估VLM构建稳健空间心理模型的能力，包括位置（认知图谱）、方向（透视）和动态（心理模拟）。研究探索了三种方法来帮助VLM近似空间心理模型：未见过的中间视图、自然语言推理链和认知图谱。最终采用了一种协同方法——“先地图后推理”，该方法联合训练模型，使其首先生成认知图谱，然后在此基础上进行推理。此外，还加入了强化学习以进一步提升性能。

Result: 在MindCube基准测试中，现有VLMs表现出接近随机的性能。通过“先地图后推理”方法，准确率从37.8%显著提升到60.8%（+23.0%）。加入强化学习后，性能进一步提升至70.7%（+32.9%）。

Conclusion: 本文的关键见解是，通过主动构建和利用内部结构化的空间表示，结合灵活的推理过程，对空间心理模型进行“支架式”构建，可以显著提高视觉语言模型对不可观测空间的理解。

Abstract: Can Vision Language Models (VLMs) imagine the full scene from just a few
views, like humans do? Humans form spatial mental models, internal
representations of unseen space, to reason about layout, perspective, and
motion. Our new MindCube benchmark with 21,154 questions across 3,268 images
exposes this critical gap, where existing VLMs exhibit near-random performance.
Using MindCube, we systematically evaluate how well VLMs build robust spatial
mental models through representing positions (cognitive mapping), orientations
(perspective-taking), and dynamics (mental simulation for "what-if" movements).
We then explore three approaches to help VLMs approximate spatial mental
models, including unseen intermediate views, natural language reasoning chains,
and cognitive maps. The significant improvement comes from a synergistic
approach, "map-then-reason", that jointly trains the model to first generate a
cognitive map and then reason upon it. By training models to reason over these
internal maps, we boosted accuracy from 37.8% to 60.8% (+23.0%). Adding
reinforcement learning pushed performance even further to 70.7% (+32.9%). Our
key insight is that such scaffolding of spatial mental models, actively
constructing and utilizing internal structured spatial representations with
flexible reasoning processes, significantly improves understanding of
unobservable space.

</details>


### [55] [Mind2Web 2: Evaluating Agentic Search with Agent-as-a-Judge](https://arxiv.org/abs/2506.21506)
*Boyu Gou,Zanming Huang,Yuting Ning,Yu Gu,Michael Lin,Weijian Qi,Andrei Kopanev,Botao Yu,Bernal Jiménez Gutiérrez,Yiheng Shu,Chan Hee Song,Jiaman Wu,Shijie Chen,Hanane Nour Moussa,Tianshu Zhang,Jian Xie,Yifei Li,Tianci Xue,Zeyi Liao,Kai Zhang,Boyuan Zheng,Zhaowei Cai,Viktor Rozgic,Morteza Ziyadi,Huan Sun,Yu Su*

Main category: cs.AI

TL;DR: 该论文提出了Mind2Web 2基准和Agent-as-a-Judge评估框架，用于评估复杂的代理搜索系统，并发现当前顶级系统已能达到人类表现的50-70%。


<details>
  <summary>Details</summary>
Motivation: 现有的评估基准和方法已无法满足日益复杂和开放的代理搜索系统的需求。

Method: 本文引入了Mind2Web 2，一个包含130个真实、高质量、长周期任务的基准，并提出了新颖的Agent-as-a-Judge框架，通过构建基于树状评估标准的任务特定判官代理来自动评估答案的正确性和来源归因。

Result: 对九个前沿代理搜索系统进行评估发现，表现最佳的OpenAI Deep Research系统能以一半的时间达到人类性能的50-70%。

Conclusion: Mind2Web 2为开发和评估下一代代理搜索系统提供了严格的基础。

Abstract: Agentic search such as Deep Research systems, where large language models
autonomously browse the web, synthesize information, and return comprehensive
citation-backed answers, represents a major shift in how users interact with
web-scale information. While promising greater efficiency and cognitive
offloading, the growing complexity and open-endedness of agentic search have
outpaced existing evaluation benchmarks and methodologies, which largely assume
short search horizons and static answers. In this paper, we introduce Mind2Web
2, a benchmark of 130 realistic, high-quality, and long-horizon tasks that
require real-time web browsing and extensive information synthesis, constructed
with over 1,000 hours of human labor. To address the challenge of evaluating
time-varying and complex answers, we propose a novel Agent-as-a-Judge
framework. Our method constructs task-specific judge agents based on a
tree-structured rubric design to automatically assess both answer correctness
and source attribution. We conduct a comprehensive evaluation of nine frontier
agentic search systems and human performance, along with a detailed error
analysis to draw insights for future development. The best-performing system,
OpenAI Deep Research, can already achieve 50-70% of human performance while
spending half the time, showing a great potential. Altogether, Mind2Web 2
provides a rigorous foundation for developing and benchmarking the next
generation of agentic search systems.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [56] [Hybrid Deep Learning and Signal Processing for Arabic Dialect Recognition in Low-Resource Settings](https://arxiv.org/abs/2506.21386)
*Ghazal Al-Shwayyat,Omer Nezih Gerek*

Main category: eess.AS

TL;DR: 本研究旨在解决资源受限环境下阿拉伯语方言识别的挑战。论文开发并评估了两种混合模型：MFCC+CNN和DWT+RNN。实验结果表明，MFCC+CNN架构表现优异，准确率达到91.2%，显著优于DWT+RNN。


<details>
  <summary>Details</summary>
Motivation: 由于阿拉伯语的语言多样性以及大型标注数据集的稀缺性，特别是对于代表性不足的方言，阿拉伯语方言识别在语音技术中面临重大挑战。本研究旨在低资源场景下解决这一问题。

Method: 本研究调查了混合建模策略，将经典信号处理技术与深度学习架构相结合。开发并评估了两种混合模型：1)梅尔频率倒谱系数(MFCC)结合卷积神经网络(CNN)，以及2)离散小波变换(DWT)特征结合循环神经网络(RNN)。模型在Common Voice阿拉伯语数据集的方言过滤子集上进行训练。

Result: 实验结果表明，MFCC+CNN架构取得了卓越的性能，准确率达91.2%，并具有强大的精确度、召回率和F1分数，显著优于小波+RNN配置(其准确率为66.5%)。这些发现强调了利用谱特征和卷积模型进行阿拉伯语方言识别的有效性，特别是在有限标注数据的情况下。

Conclusion: 本研究为资源受限环境下阿拉伯语方言识别的未来发展奠定了坚实的基础。研究还指出了数据集大小、标签中潜在的区域重叠以及模型优化方面的局限性，为未来的研究提供了路线图。未来的改进建议包括采用更大的标注语料库、整合自监督学习技术以及探索更先进的神经网络架构，如Transformer。

Abstract: Arabic dialect recognition presents a significant challenge in speech
technology due to the linguistic diversity of Arabic and the scarcity of large
annotated datasets, particularly for underrepresented dialects. This research
investigates hybrid modeling strategies that integrate classical signal
processing techniques with deep learning architectures to address this problem
in low-resource scenarios. Two hybrid models were developed and evaluated: (1)
Mel-Frequency Cepstral Coefficients (MFCC) combined with a Convolutional Neural
Network (CNN), and (2) Discrete Wavelet Transform (DWT) features combined with
a Recurrent Neural Network (RNN). The models were trained on a dialect-filtered
subset of the Common Voice Arabic dataset, with dialect labels assigned based
on speaker metadata. Experimental results demonstrate that the MFCC + CNN
architecture achieved superior performance, with an accuracy of 91.2% and
strong precision, recall, and F1-scores, significantly outperforming the
Wavelet + RNN configuration, which achieved an accuracy of 66.5%. These
findings highlight the effectiveness of leveraging spectral features with
convolutional models for Arabic dialect recognition, especially when working
with limited labeled data. The study also identifies limitations related to
dataset size, potential regional overlaps in labeling, and model optimization,
providing a roadmap for future research. Recommendations for further
improvement include the adoption of larger annotated corpora, integration of
self-supervised learning techniques, and exploration of advanced neural
architectures such as Transformers. Overall, this research establishes a strong
baseline for future developments in Arabic dialect recognition within
resource-constrained environments.

</details>
