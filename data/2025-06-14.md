<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [A Survey of Automatic Evaluation Methods on Text, Visual and Speech Generations](https://arxiv.org/abs/2506.10019)
*Tian Lan,Yang-Hao Zhou,Zi-Ao Ma,Fanshu Sun,Rui-Qing Sun,Junyu Luo,Rong-Cheng Tu,Heyan Huang,Chen Xu,Zhijing Wu,Xian-Ling Mao*

Main category: cs.CL

TL;DR: 本文提出了一个跨模态生成内容自动评估方法的统一分类框架。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏一个系统性的框架来组织跨文本、视觉和音频模态的生成内容自动评估方法，尽管深度学习在生成式AI方面取得了显著进展，但评估生成内容质量仍具挑战。

Method: 本文提出了一个统一的评估方法分类框架，涵盖文本、图像和音频模态，并识别了五种基本评估范式。

Result: 该框架首先审视了文本生成评估方法，然后将其扩展到图像和音频生成，展示了其广泛适用性。

Conclusion: 论文讨论了跨模态评估方法的未来研究方向。

Abstract: Recent advances in deep learning have significantly enhanced generative AI
capabilities across text, images, and audio. However, automatically evaluating
the quality of these generated outputs presents ongoing challenges. Although
numerous automatic evaluation methods exist, current research lacks a
systematic framework that comprehensively organizes these methods across text,
visual, and audio modalities. To address this issue, we present a comprehensive
review and a unified taxonomy of automatic evaluation methods for generated
content across all three modalities; We identify five fundamental paradigms
that characterize existing evaluation approaches across these domains. Our
analysis begins by examining evaluation methods for text generation, where
techniques are most mature. We then extend this framework to image and audio
generation, demonstrating its broad applicability. Finally, we discuss
promising directions for future research in cross-modal evaluation
methodologies.

</details>
