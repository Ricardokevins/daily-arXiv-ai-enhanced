{"id": "2506.04344", "pdf": "https://arxiv.org/pdf/2506.04344", "abs": "https://arxiv.org/abs/2506.04344", "authors": ["Caojin Zhang", "Qiang Zhang", "Ke Li", "Sai Vidyaranya Nuthalapati", "Benyu Zhang", "Jason Liu", "Serena Li", "Lizhu Zhang", "Xiangjun Fan"], "title": "GEM: Empowering LLM for both Embedding Generation and Language Understanding", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large decoder-only language models (LLMs) have achieved remarkable success in\ngeneration and reasoning tasks, where they generate text responses given\ninstructions. However, many applications, e.g., retrieval augmented generation\n(RAG), still rely on separate embedding models to generate text embeddings,\nwhich can complicate the system and introduce discrepancies in understanding of\nthe query between the embedding model and LLMs. To address this limitation, we\npropose a simple self-supervised approach, Generative Embedding large language\nModel (GEM), that enables any large decoder-only LLM to generate high-quality\ntext embeddings while maintaining its original text generation and reasoning\ncapabilities. Our method inserts new special token(s) into a text body, and\ngenerates summarization embedding of the text by manipulating the attention\nmask. This method could be easily integrated into post-training or fine tuning\nstages of any existing LLMs. We demonstrate the effectiveness of our approach\nby applying it to two popular LLM families, ranging from 1B to 8B parameters,\nand evaluating the transformed models on both text embedding benchmarks (MTEB)\nand NLP benchmarks (MMLU). The results show that our proposed method\nsignificantly improves the original LLMs on MTEB while having a minimal impact\non MMLU. Our strong results indicate that our approach can empower LLMs with\nstate-of-the-art text embedding capabilities while maintaining their original\nNLP performance", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGEM\u7684\u81ea\u76d1\u7763\u65b9\u6cd5\uff0c\u4f7f\u5927\u578b\u89e3\u7801\u5668LLM\u80fd\u591f\u5728\u4fdd\u6301\u5176\u539f\u59cb\u6587\u672c\u751f\u6210\u548c\u63a8\u7406\u80fd\u529b\u7684\u540c\u65f6\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u6587\u672c\u5d4c\u5165\u3002", "motivation": "\u73b0\u6709\u7684LLMs\u5728\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u9700\u8981\u6587\u672c\u5d4c\u5165\u7684\u5e94\u7528\uff08\u5982RAG\uff09\u4e2d\uff0c\u4ecd\u4f9d\u8d56\u72ec\u7acb\u7684\u5d4c\u5165\u6a21\u578b\uff0c\u8fd9\u4f1a\u4f7f\u7cfb\u7edf\u590d\u6742\u5316\uff0c\u5e76\u53ef\u80fd\u5bfc\u81f4\u7406\u89e3\u4e0a\u7684\u5dee\u5f02\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u751f\u6210\u5f0f\u5d4c\u5165\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08GEM\uff09\u201d\u7684\u81ea\u76d1\u7763\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u5728\u6587\u672c\u4e3b\u4f53\u4e2d\u63d2\u5165\u65b0\u7684\u7279\u6b8atoken\u5e76\u64cd\u7eb5\u6ce8\u610f\u529b\u63a9\u7801\u6765\u751f\u6210\u6587\u672c\u7684\u6458\u8981\u5d4c\u5165\u3002\u53ef\u8f7b\u677e\u96c6\u6210\u5230\u73b0\u6709LLM\u7684\u540e\u8bad\u7ec3\u6216\u5fae\u8c03\u9636\u6bb5\u3002", "result": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86LLMs\u5728MTEB\uff08\u6587\u672c\u5d4c\u5165\u57fa\u51c6\uff09\u4e0a\u7684\u8868\u73b0\uff0c\u540c\u65f6\u5bf9MMLU\uff08NLP\u57fa\u51c6\uff09\u7684\u5f71\u54cd\u6700\u5c0f\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u8d4b\u4e88LLMs\u6700\u5148\u8fdb\u7684\u6587\u672c\u5d4c\u5165\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u5176\u539f\u59cb\u7684NLP\u6027\u80fd\u3002"}}
