<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [ClimateChat: Designing Data and Methods for Instruction Tuning LLMs to Answer Climate Change Queries](https://arxiv.org/abs/2506.13796)
*Zhou Chen,Xiao Wang,Yuanhong Liao,Ming Lin,Yuqi Bai*

Main category: cs.CL

TL;DR: 本研究提出了一种自动化方法，用于构建高精度气候变化指令数据，并用其微调大型语言模型，创建了ClimateChat，显著提升了气候变化问答任务的性能。


<details>
  <summary>Details</summary>
Motivation: 全球气候变化问题日益严峻，对气候科学研究的需求持续增长。当前研究在高效生成大量高精度气候变化指令数据方面不足，这限制了气候变化大型语言模型的进一步发展。

Method: 本研究引入了一种自动化构建指令数据的方法。该方法利用文档中的事实和背景知识生成指令，并通过网络爬取和收集种子指令来增强指令数据的多样性。利用此方法构建了气候变化指令数据集ClimateChat-Corpus，并用其微调开源大型语言模型，从而产生了ClimateChat模型。

Result: ClimateChat在气候变化问答任务上显著提升了性能。研究还评估了不同基础模型和指令数据对大型语言模型性能的影响，并证明了其适应广泛气候变化科学发现任务的能力。

Conclusion: 这项研究为构建气候变化指令数据和训练气候变化专用大型语言模型提供了有价值的参考和经验支持，并强调了为指令微调选择合适基础模型的重要性。

Abstract: As the issue of global climate change becomes increasingly severe, the demand for research in climate science continues to grow. Natural language processing technologies, represented by Large Language Models (LLMs), have been widely applied to climate change-specific research, providing essential information support for decision-makers and the public. Some studies have improved model performance on relevant tasks by constructing climate change-related instruction data and instruction-tuning LLMs. However, current research remains inadequate in efficiently producing large volumes of high-precision instruction data for climate change, which limits further development of climate change LLMs. This study introduces an automated method for constructing instruction data. The method generates instructions using facts and background knowledge from documents and enhances the diversity of the instruction data through web scraping and the collection of seed instructions. Using this method, we constructed a climate change instruction dataset, named ClimateChat-Corpus, which was used to fine-tune open-source LLMs, resulting in an LLM named ClimateChat. Evaluation results show that ClimateChat significantly improves performance on climate change question-and-answer tasks. Additionally, we evaluated the impact of different base models and instruction data on LLM performance and demonstrated its capability to adapt to a wide range of climate change scientific discovery tasks, emphasizing the importance of selecting an appropriate base model for instruction tuning. This research provides valuable references and empirical support for constructing climate change instruction data and training climate change-specific LLMs.

</details>
