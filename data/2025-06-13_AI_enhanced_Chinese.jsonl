{"id": "2506.10019", "pdf": "https://arxiv.org/pdf/2506.10019", "abs": "https://arxiv.org/abs/2506.10019", "authors": ["Tian Lan", "Yang-Hao Zhou", "Zi-Ao Ma", "Fanshu Sun", "Rui-Qing Sun", "Junyu Luo", "Rong-Cheng Tu", "Heyan Huang", "Chen Xu", "Zhijing Wu", "Xian-Ling Mao"], "title": "A Survey of Automatic Evaluation Methods on Text, Visual and Speech Generations", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Recent advances in deep learning have significantly enhanced generative AI\ncapabilities across text, images, and audio. However, automatically evaluating\nthe quality of these generated outputs presents ongoing challenges. Although\nnumerous automatic evaluation methods exist, current research lacks a\nsystematic framework that comprehensively organizes these methods across text,\nvisual, and audio modalities. To address this issue, we present a comprehensive\nreview and a unified taxonomy of automatic evaluation methods for generated\ncontent across all three modalities; We identify five fundamental paradigms\nthat characterize existing evaluation approaches across these domains. Our\nanalysis begins by examining evaluation methods for text generation, where\ntechniques are most mature. We then extend this framework to image and audio\ngeneration, demonstrating its broad applicability. Finally, we discuss\npromising directions for future research in cross-modal evaluation\nmethodologies.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u5168\u9762\u56de\u987e\u5e76\u7edf\u4e00\u4e86\u6587\u672c\u3001\u56fe\u50cf\u548c\u97f3\u9891\u751f\u6210\u5185\u5bb9\u7684\u81ea\u52a8\u8bc4\u4f30\u65b9\u6cd5\uff0c\u63d0\u51fa\u4e86\u4e94\u79cd\u57fa\u672c\u8303\u5f0f\uff0c\u5e76\u8ba8\u8bba\u4e86\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u5c3d\u7ba1\u6df1\u5ea6\u5b66\u4e60\u5728\u751f\u6210\u5f0fAI\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u81ea\u52a8\u8bc4\u4f30\u751f\u6210\u5185\u5bb9\u7684\u8d28\u91cf\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u4e14\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u4e00\u4e2a\u7cfb\u7edf\u6027\u7684\u6846\u67b6\u6765\u7ec4\u7ec7\u8de8\u6587\u672c\u3001\u89c6\u89c9\u548c\u97f3\u9891\u6a21\u6001\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u672c\u6587\u5bf9\u8de8\u6587\u672c\u3001\u89c6\u89c9\u548c\u97f3\u9891\u6a21\u6001\u7684\u751f\u6210\u5185\u5bb9\u81ea\u52a8\u8bc4\u4f30\u65b9\u6cd5\u8fdb\u884c\u4e86\u5168\u9762\u56de\u987e\u548c\u7edf\u4e00\u5206\u7c7b\u3002\u6587\u7ae0\u8bc6\u522b\u4e86\u8868\u5f81\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u7684\u4e94\u79cd\u57fa\u672c\u8303\u5f0f\uff0c\u5e76\u4ece\u6587\u672c\u751f\u6210\u8bc4\u4f30\u65b9\u6cd5\uff08\u6700\u6210\u719f\uff09\u5f00\u59cb\uff0c\u5c06\u6846\u67b6\u6269\u5c55\u5230\u56fe\u50cf\u548c\u97f3\u9891\u751f\u6210\u3002", "result": "\u5efa\u7acb\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u81ea\u52a8\u8bc4\u4f30\u65b9\u6cd5\u5206\u7c7b\u4f53\u7cfb\uff0c\u6db5\u76d6\u6587\u672c\u3001\u56fe\u50cf\u548c\u97f3\u9891\u6a21\u6001\uff0c\u5e76\u8bc6\u522b\u4e86\u4e94\u79cd\u57fa\u672c\u8bc4\u4f30\u8303\u5f0f\uff0c\u5c55\u793a\u4e86\u5176\u5e7f\u6cdb\u9002\u7528\u6027\u3002", "conclusion": "\u8ba8\u8bba\u4e86\u8de8\u6a21\u6001\u8bc4\u4f30\u65b9\u6cd5\u672a\u6765\u7814\u7a76\u7684 promising directions\u3002"}}
{"id": "2506.10055", "pdf": "https://arxiv.org/pdf/2506.10055", "abs": "https://arxiv.org/abs/2506.10055", "authors": ["Dingfeng Shi", "Jingyi Cao", "Qianben Chen", "Weichen Sun", "Weizhen Li", "Hongxuan Lu", "Fangchen Dong", "Tianrui Qin", "King Zhu", "Minghao Yang", "Jian Yang", "Ge Zhang", "Jiaheng Liu", "Changwang Zhang", "Jun Wang", "Yuchen Eleanor Jiang", "Wangchunshu Zhou"], "title": "TaskCraft: Automated Generation of Agentic Tasks", "categories": ["cs.CL"], "comment": null, "summary": "Agentic tasks, which require multi-step problem solving with autonomy, tool\nuse, and adaptive reasoning, are becoming increasingly central to the\nadvancement of NLP and AI. However, existing instruction data lacks tool\ninteraction, and current agentic benchmarks rely on costly human annotation,\nlimiting their scalability. We introduce \\textsc{TaskCraft}, an automated\nworkflow for generating difficulty-scalable, multi-tool, and verifiable agentic\ntasks with execution trajectories. TaskCraft expands atomic tasks using\ndepth-based and width-based extensions to create structurally and\nhierarchically complex challenges. Empirical results show that these tasks\nimprove prompt optimization in the generation workflow and enhance supervised\nfine-tuning of agentic foundation models. We present a large-scale synthetic\ndataset of approximately 36,000 tasks with varying difficulty to support future\nresearch on agent tuning and evaluation.", "AI": {"tldr": "TaskCraft\u662f\u4e00\u4e2a\u81ea\u52a8\u5316\u5de5\u4f5c\u6d41\uff0c\u80fd\u751f\u6210\u53ef\u6269\u5c55\u3001\u591a\u5de5\u5177\u7684\u667a\u80fd\u4f53\u4efb\u52a1\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6570\u636e\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5e76\u6709\u52a9\u4e8e\u667a\u80fd\u4f53\u6a21\u578b\u7684\u8bad\u7ec3\u3002", "motivation": "\u73b0\u6709\u7684\u6307\u4ee4\u6570\u636e\u7f3a\u4e4f\u5de5\u5177\u4ea4\u4e92\uff0c\u5f53\u524d\u7684\u667a\u80fd\u4f53\u57fa\u51c6\u6d4b\u8bd5\u4f9d\u8d56\u4e8e\u6602\u8d35\u7684\u4eba\u5de5\u6807\u6ce8\uff0c\u9650\u5236\u4e86\u5176\u53ef\u6269\u5c55\u6027\u3002\u667a\u80fd\u4f53\u4efb\u52a1\u5728NLP\u548cAI\u9886\u57df\u53d8\u5f97\u8d8a\u6765\u8d8a\u91cd\u8981\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86TaskCraft\uff0c\u4e00\u4e2a\u81ea\u52a8\u5316\u7684\u5de5\u4f5c\u6d41\u7a0b\uff0c\u7528\u4e8e\u751f\u6210\u96be\u5ea6\u53ef\u6269\u5c55\u3001\u591a\u5de5\u5177\u4e14\u53ef\u9a8c\u8bc1\u7684\u667a\u80fd\u4f53\u4efb\u52a1\u53ca\u6267\u884c\u8f68\u8ff9\u3002\u5b83\u901a\u8fc7\u6df1\u5ea6\u548c\u5bbd\u5ea6\u6269\u5c55\u6765\u521b\u5efa\u7ed3\u6784\u548c\u5c42\u6b21\u590d\u6742\u7684\u6311\u6218\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8fd9\u4e9b\u4efb\u52a1\u6539\u8fdb\u4e86\u751f\u6210\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u7684\u63d0\u793a\u4f18\u5316\uff0c\u5e76\u589e\u5f3a\u4e86\u667a\u80fd\u4f53\u57fa\u7840\u6a21\u578b\u7684\u76d1\u7763\u5fae\u8c03\u3002\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u4e2a\u5305\u542b\u7ea636,000\u4e2a\u4e0d\u540c\u96be\u5ea6\u4efb\u52a1\u7684\u5927\u89c4\u6a21\u5408\u6210\u6570\u636e\u96c6\u3002", "conclusion": "TaskCraft\u901a\u8fc7\u63d0\u4f9b\u81ea\u52a8\u5316\u3001\u53ef\u6269\u5c55\u7684\u667a\u80fd\u4f53\u4efb\u52a1\u751f\u6210\u65b9\u6848\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6570\u636e\u548c\u57fa\u51c6\u6d4b\u8bd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u88ab\u8bc1\u660e\u5bf9\u63d0\u793a\u4f18\u5316\u548c\u667a\u80fd\u4f53\u6a21\u578b\u5fae\u8c03\u6709\u76ca\uff0c\u540c\u65f6\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u6570\u636e\u96c6\u3002"}}
{"id": "2506.10077", "pdf": "https://arxiv.org/pdf/2506.10077", "abs": "https://arxiv.org/abs/2506.10077", "authors": ["Christopher J. Agostino", "Quan Le Thien", "Molly Apsel", "Denizhan Pak", "Elina Lesyk", "Ashabari Majumdar"], "title": "A quantum semantic framework for natural language processing", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.IT", "math.IT"], "comment": "12 pages, 2 figures, accepted submission to Quantum AI and NLP 2025", "summary": "Semantic degeneracy represents a fundamental property of natural language\nthat extends beyond simple polysemy to encompass the combinatorial explosion of\npotential interpretations that emerges as semantic expressions increase in\ncomplexity. Large Language Models (LLMs) and other modern NLP systems face\ninherent limitations precisely because they operate within natural language\nitself, making them subject to the same interpretive constraints imposed by\nsemantic degeneracy. In this work, we argue using Kolmogorov complexity that as\nan expression's complexity grows, the likelihood of any interpreting agent\n(human or LLM-powered AI) recovering the single intended meaning vanishes. This\ncomputational intractability suggests the classical view that linguistic forms\npossess meaning in and of themselves is flawed. We alternatively posit that\nmeaning is instead actualized through an observer-dependent interpretive act.\nTo test this, we conducted a semantic Bell inequality test using diverse LLM\nagents as ``computational cognitive systems'' to interpret ambiguous word pairs\nunder varied contextual settings. Across several independent experiments, we\nfound average CHSH expectation values ranging from 1.2 to 2.8, with several\nruns yielding values (e.g., 2.3-2.4) that significantly violate the classical\nboundary ($|S|\\leq2$). This demonstrates that linguistic interpretation under\nambiguity can exhibit non-classical contextuality, consistent with results from\nhuman cognition experiments. These results inherently imply that classical\nfrequentist-based analytical approaches for natural language are necessarily\nlossy. Instead, we propose that Bayesian-style repeated sampling approaches can\nprovide more practically useful and appropriate characterizations of linguistic\nmeaning in context.", "AI": {"tldr": "\u8bed\u4e49\u9000\u5316\u4f7f\u5f97LLM\u96be\u4ee5\u6062\u590d\u5355\u4e00\u610f\u4e49\u3002\u672c\u6587\u901a\u8fc7\u5bf9LLM\u8fdb\u884c\u8bed\u4e49\u8d1d\u5c14\u4e0d\u7b49\u5f0f\u6d4b\u8bd5\uff0c\u8868\u660e\u8bed\u8a00\u89e3\u91ca\u5177\u6709\u89c2\u5bdf\u8005\u4f9d\u8d56\u6027\uff0c\u5e76\u5c55\u73b0\u51fa\u975e\u7ecf\u5178\u8bed\u5883\u6027\uff0c\u63d0\u51fa\u8d1d\u53f6\u65af\u65b9\u6cd5\u4f18\u4e8e\u7ecf\u5178\u9891\u7387\u5b66\u65b9\u6cd5\u6765\u8868\u5f81\u8bed\u8a00\u610f\u4e49\u3002", "motivation": "\u62bd\u8c61\u6307\u51fa\u8bed\u4e49\u9000\u5316\u662f\u81ea\u7136\u8bed\u8a00\u7684\u57fa\u672c\u5c5e\u6027\uff0c\u968f\u7740\u8bed\u4e49\u8868\u8fbe\u590d\u6742\u6027\u7684\u589e\u52a0\uff0c\u89e3\u91ca\u4ee3\u7406\uff08\u5305\u62ec\u4eba\u7c7b\u548cLLM\uff09\u6062\u590d\u5355\u4e00\u9884\u671f\u610f\u4e49\u7684\u53ef\u80fd\u6027\u4f1a\u6d88\u5931\uff0c\u8fd9\u6311\u6218\u4e86\u8bed\u8a00\u5f62\u5f0f\u672c\u8eab\u5177\u6709\u610f\u4e49\u7684\u7ecf\u5178\u89c2\u70b9\u3002", "method": "\u8fd0\u7528\u67ef\u5c14\u83ab\u54e5\u7f57\u592b\u590d\u6742\u6027\u7406\u8bba\uff0c\u8bba\u8bc1\u968f\u7740\u8868\u8fbe\u590d\u6742\u6027\u589e\u52a0\uff0c\u89e3\u91ca\u4ee3\u7406\u6062\u590d\u5355\u4e00\u9884\u671f\u610f\u4e49\u7684\u53ef\u80fd\u6027\u6d88\u5931\uff1b\u63d0\u51fa\u610f\u4e49\u662f\u901a\u8fc7\u4f9d\u8d56\u4e8e\u89c2\u5bdf\u8005\u7684\u89e3\u91ca\u884c\u4e3a\u5b9e\u73b0\u7684\uff1b\u4f7f\u7528\u4e0d\u540c\u7684LLM\u4ee3\u7406\u4f5c\u4e3a\u201c\u8ba1\u7b97\u8ba4\u77e5\u7cfb\u7edf\u201d\uff0c\u5728\u4e0d\u540c\u7684\u8bed\u5883\u8bbe\u7f6e\u4e0b\u89e3\u91ca\u6a21\u7cca\u7684\u8bcd\u5bf9\uff0c\u8fdb\u884c\u8bed\u4e49\u8d1d\u5c14\u4e0d\u7b49\u5f0f\u6d4b\u8bd5\u3002", "result": "\u5e73\u5747CHSH\u671f\u671b\u503c\u8303\u56f4\u4e3a1.2\u52302.8\uff1b\u591a\u4e2a\u8fd0\u884c\u7ed3\u679c\uff08\u4f8b\u59822.3-2.4\uff09\u663e\u8457\u8fdd\u53cd\u4e86\u7ecf\u5178\u8fb9\u754c($|S|\\leq2$)\uff1b\u8fd9\u8868\u660e\u5728\u6a21\u7cca\u6027\u4e0b\u7684\u8bed\u8a00\u89e3\u91ca\u53ef\u4ee5\u8868\u73b0\u51fa\u975e\u7ecf\u5178\u7684\u8bed\u5883\u4f9d\u8d56\u6027\uff0c\u4e0e\u4eba\u7c7b\u8ba4\u77e5\u5b9e\u9a8c\u7ed3\u679c\u4e00\u81f4\u3002", "conclusion": "\u57fa\u4e8e\u7ecf\u5178\u9891\u7387\u8bba\u7684\u81ea\u7136\u8bed\u8a00\u5206\u6790\u65b9\u6cd5\u5fc5\u7136\u662f\u6709\u635f\u7684\uff1b\u8d1d\u53f6\u65af\u98ce\u683c\u7684\u91cd\u590d\u91c7\u6837\u65b9\u6cd5\u53ef\u4ee5\u4e3a\u4e0a\u4e0b\u6587\u4e2d\u7684\u8bed\u8a00\u610f\u4e49\u63d0\u4f9b\u66f4\u5b9e\u7528\u548c\u6070\u5f53\u7684\u8868\u5f81\u3002"}}
{"id": "2506.10086", "pdf": "https://arxiv.org/pdf/2506.10086", "abs": "https://arxiv.org/abs/2506.10086", "authors": ["Christodoulos Constantinides", "Shuxin Lin", "Nianjun Zhou", "Dhaval Patel"], "title": "Chat-of-Thought: Collaborative Multi-Agent System for Generating Domain Specific Information", "categories": ["cs.CL"], "comment": null, "summary": "This paper presents a novel multi-agent system called Chat-of-Thought,\ndesigned to facilitate the generation of Failure Modes and Effects Analysis\n(FMEA) documents for industrial assets. Chat-of-Thought employs multiple\ncollaborative Large Language Model (LLM)-based agents with specific roles,\nleveraging advanced AI techniques and dynamic task routing to optimize the\ngeneration and validation of FMEA tables. A key innovation in this system is\nthe introduction of a Chat of Thought, where dynamic, multi-persona-driven\ndiscussions enable iterative refinement of content. This research explores the\napplication domain of industrial equipment monitoring, highlights key\nchallenges, and demonstrates the potential of Chat-of-Thought in addressing\nthese challenges through interactive, template-driven workflows and\ncontext-aware agent collaboration.", "AI": {"tldr": "Chat-of-Thought\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53LLM\u7cfb\u7edf\uff0c\u901a\u8fc7\u591a\u89d2\u8272\u8ba8\u8bba\u548c\u52a8\u6001\u4efb\u52a1\u8def\u7531\uff0c\u4f18\u5316\u5de5\u4e1aFMEA\u6587\u6863\u7684\u751f\u6210\u548c\u9a8c\u8bc1\u3002", "motivation": "\u89e3\u51b3\u5de5\u4e1a\u8d44\u4ea7\u6545\u969c\u6a21\u5f0f\u4e0e\u5f71\u54cd\u5206\u6790\uff08FMEA\uff09\u6587\u6863\u751f\u6210\u4e2d\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edfChat-of-Thought\uff0c\u8be5\u7cfb\u7edf\u5229\u7528\u591a\u4e2a\u57fa\u4e8eLLM\u7684\u534f\u4f5c\u667a\u80fd\u4f53\uff0c\u901a\u8fc7\u7279\u5b9a\u89d2\u8272\u3001\u5148\u8fdbAI\u6280\u672f\u548c\u52a8\u6001\u4efb\u52a1\u8def\u7531\u6765\u4f18\u5316FMEA\u8868\u683c\u7684\u751f\u6210\u548c\u9a8c\u8bc1\u3002\u6838\u5fc3\u521b\u65b0\u662f\u5f15\u5165\u201c\u601d\u7ef4\u804a\u5929\u201d\uff0c\u901a\u8fc7\u52a8\u6001\u3001\u591a\u89d2\u8272\u9a71\u52a8\u7684\u8ba8\u8bba\u5b9e\u73b0\u5185\u5bb9\u7684\u8fed\u4ee3\u5b8c\u5584\u3002", "result": "Chat-of-Thought\u7cfb\u7edf\u80fd\u591f\u4f18\u5316FMEA\u8868\u683c\u7684\u751f\u6210\u548c\u9a8c\u8bc1\uff0c\u5e76\u901a\u8fc7\u4ea4\u4e92\u5f0f\u3001\u6a21\u677f\u9a71\u52a8\u7684\u5de5\u4f5c\u6d41\u7a0b\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u667a\u80fd\u4f53\u534f\u4f5c\uff0c\u5728\u5de5\u4e1a\u8bbe\u5907\u76d1\u63a7\u9886\u57df\u5c55\u793a\u4e86\u89e3\u51b3\u6311\u6218\u7684\u6f5c\u529b\u3002", "conclusion": "Chat-of-Thought\u7cfb\u7edf\u901a\u8fc7\u5176\u72ec\u7279\u7684\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u548c\u8fed\u4ee3\u5b8c\u5584\u673a\u5236\uff0c\u4e3a\u5de5\u4e1a\u8d44\u4ea7FMEA\u6587\u6863\u7684\u751f\u6210\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u4e14\u6709\u6f5c\u529b\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.10095", "pdf": "https://arxiv.org/pdf/2506.10095", "abs": "https://arxiv.org/abs/2506.10095", "authors": ["Xiao Li", "Joel Kreuzwieser", "Alan Peters"], "title": "When Meaning Stays the Same, but Models Drift: Evaluating Quality of Service under Token-Level Behavioral Instability in LLMs", "categories": ["cs.CL"], "comment": "This paper was developed for presentation at ICML 2025 Tokshop\n  Workshop, but is now submitted as a standalone contribution", "summary": "We investigate how large language models respond to prompts that differ only\nin their token-level realization but preserve the same semantic intent, a\nphenomenon we call prompt variance. We propose Prompt-Based Semantic Shift\n(PBSS), a diagnostic framework for measuring behavioral drift in LLMs under\nsemantically equivalent prompt rewordings. Applied to ten constrained tasks,\nPBSS reveals consistent, model-specific response shifts, suggesting statistical\nregularities linked to tokenization and decoding. These results highlight an\noverlooked dimension of model evaluation stability under rephrasing and suggest\nthat tokenization strategies and decoding dynamics may contribute to\npost-training quality of service instability.", "AI": {"tldr": "\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u8bed\u4e49\u76f8\u540c\u4f46\u8868\u5f81\u4e0d\u540c\u7684\u63d0\u793a\u7b26\u8868\u73b0\u51fa\u884c\u4e3a\u6f02\u79fb\u3002\u672c\u6587\u63d0\u51fa\u4e86PBSS\u6846\u67b6\u6765\u8861\u91cf\u8fd9\u79cd\u6f02\u79fb\uff0c\u53d1\u73b0\u6a21\u578b\u5728\u5206\u8bcd\u548c\u89e3\u7801\u65b9\u9762\u5b58\u5728\u4e00\u81f4\u7684\u54cd\u5e94\u53d8\u5316\uff0c\u8fd9\u63ed\u793a\u4e86\u6a21\u578b\u8bc4\u4f30\u7a33\u5b9a\u6027\u7684\u4e00\u4e2a\u88ab\u5ffd\u89c6\u7684\u7ef4\u5ea6\u3002", "motivation": "\u7814\u7a76\u5927\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u54cd\u5e94\u4ec5\u5728\u5206\u8bcd\u5c42\u9762\u4e0d\u540c\u4f46\u8bed\u4e49\u610f\u56fe\u76f8\u540c\u7684\u63d0\u793a\u7b26\uff08\u5373\u63d0\u793a\u7b26\u53d8\u5f02\u73b0\u8c61\uff09\u3002", "method": "\u63d0\u51fa\u4e86\u201c\u57fa\u4e8e\u63d0\u793a\u7b26\u7684\u8bed\u4e49\u504f\u79fb\u201d\uff08PBSS\uff09\u8bca\u65ad\u6846\u67b6\uff0c\u7528\u4e8e\u8861\u91cf\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8bed\u4e49\u7b49\u6548\u7684\u63d0\u793a\u7b26\u91cd\u8ff0\u4e0b\u7684\u884c\u4e3a\u6f02\u79fb\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u5341\u4e2a\u53d7\u9650\u4efb\u52a1\u3002", "result": "PBSS\u63ed\u793a\u4e86\u6301\u7eed\u7684\u3001\u6a21\u578b\u7279\u5b9a\u7684\u54cd\u5e94\u504f\u79fb\uff0c\u8fd9\u8868\u660e\u4e0e\u5206\u8bcd\u548c\u89e3\u7801\u76f8\u5173\u7684\u7edf\u8ba1\u89c4\u5f8b\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u7a81\u51fa\u4e86\u6a21\u578b\u5728\u590d\u8ff0\u4e0b\u8bc4\u4f30\u7a33\u5b9a\u6027\u7684\u4e00\u4e2a\u88ab\u5ffd\u89c6\u7684\u7ef4\u5ea6\uff0c\u5e76\u8868\u660e\u5206\u8bcd\u7b56\u7565\u548c\u89e3\u7801\u52a8\u6001\u53ef\u80fd\u5bfc\u81f4\u8bad\u7ec3\u540e\u7684\u670d\u52a1\u8d28\u91cf\u4e0d\u7a33\u5b9a\u3002"}}
{"id": "2506.10116", "pdf": "https://arxiv.org/pdf/2506.10116", "abs": "https://arxiv.org/abs/2506.10116", "authors": ["Caijun Jia", "Nan Xu", "Jingxuan Wei", "Qingli Wang", "Lei Wang", "Bihui Yu", "Junnan Zhu"], "title": "ChartReasoner: Code-Driven Modality Bridging for Long-Chain Reasoning in Chart Question Answering", "categories": ["cs.CL"], "comment": null, "summary": "Recently, large language models have shown remarkable reasoning capabilities\nthrough long-chain reasoning before responding. However, how to extend this\ncapability to visual reasoning tasks remains an open challenge. Existing\nmultimodal reasoning approaches transfer such visual reasoning task into\ntextual reasoning task via several image-to-text conversions, which often lose\ncritical structural and semantic information embedded in visualizations,\nespecially for tasks like chart question answering that require a large amount\nof visual details. To bridge this gap, we propose ChartReasoner, a code-driven\nnovel two-stage framework designed to enable precise, interpretable reasoning\nover charts. We first train a high-fidelity model to convert diverse chart\nimages into structured ECharts codes, preserving both layout and data semantics\nas lossless as possible. Then, we design a general chart reasoning data\nsynthesis pipeline, which leverages this pretrained transport model to\nautomatically and scalably generate chart reasoning trajectories and utilizes a\ncode validator to filter out low-quality samples. Finally, we train the final\nmultimodal model using a combination of supervised fine-tuning and\nreinforcement learning on our synthesized chart reasoning dataset and\nexperimental results on four public benchmarks clearly demonstrate the\neffectiveness of our proposed ChartReasoner. It can preserve the original\ndetails of the charts as much as possible and perform comparably with\nstate-of-the-art open-source models while using fewer parameters, approaching\nthe performance of proprietary systems like GPT-4o in out-of-domain settings.", "AI": {"tldr": "ChartReasoner\u662f\u4e00\u4e2a\u4ee3\u7801\u9a71\u52a8\u7684\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u56fe\u8868\u56fe\u50cf\u8f6c\u6362\u4e3aECharts\u4ee3\u7801\u6765\u63d0\u9ad8\u89c6\u89c9\u63a8\u7406\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u56fe\u8868\u7ec6\u8282\u4fdd\u7559\u4e0a\u7684\u4e0d\u8db3\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u4e0e\u9876\u5c16\u6a21\u578b\u76f8\u5ab2\u7f8e\u7684\u6027\u80fd\uff0c\u540c\u65f6\u53c2\u6570\u66f4\u5c11\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u56fe\u8868\u89c6\u89c9\u63a8\u7406\u4e2d\u901a\u8fc7\u56fe\u50cf\u5230\u6587\u672c\u7684\u8f6c\u6362\u4f1a\u4e22\u5931\u5173\u952e\u7684\u7ed3\u6784\u548c\u8bed\u4e49\u4fe1\u606f\uff0c\u5bfc\u81f4\u65e0\u6cd5\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u957f\u94fe\u63a8\u7406\u80fd\u529b\u6709\u6548\u6269\u5c55\u5230\u89c6\u89c9\u63a8\u7406\u4efb\u52a1\u3002", "method": "\u63d0\u51faChartReasoner\uff0c\u4e00\u4e2a\u4ee3\u7801\u9a71\u52a8\u7684\u4e24\u9636\u6bb5\u6846\u67b6\u3002\u9996\u5148\uff0c\u8bad\u7ec3\u9ad8\u4fdd\u771f\u6a21\u578b\u5c06\u56fe\u8868\u56fe\u50cf\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u7684ECharts\u4ee3\u7801\u3002\u5176\u6b21\uff0c\u8bbe\u8ba1\u4e00\u4e2a\u901a\u7528\u7684\u56fe\u8868\u63a8\u7406\u6570\u636e\u5408\u6210\u7ba1\u7ebf\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u7684\u4f20\u8f93\u6a21\u578b\u81ea\u52a8\u751f\u6210\u63a8\u7406\u8f68\u8ff9\u5e76\u4f7f\u7528\u4ee3\u7801\u9a8c\u8bc1\u5668\u8fc7\u6ee4\u4f4e\u8d28\u91cf\u6837\u672c\u3002\u6700\u540e\uff0c\u7ed3\u5408\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u5728\u5408\u6210\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u6700\u7ec8\u7684\u591a\u6a21\u6001\u6a21\u578b\u3002", "result": "\u5728\u56db\u4e2a\u516c\u5171\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u6e05\u6670\u5730\u8bc1\u660e\u4e86ChartReasoner\u7684\u6709\u6548\u6027\u3002\u5b83\u80fd\u6700\u5927\u7a0b\u5ea6\u4fdd\u7559\u56fe\u8868\u539f\u59cb\u7ec6\u8282\uff0c\u5e76\u4ee5\u66f4\u5c11\u7684\u53c2\u6570\u8fbe\u5230\u4e0e\u6700\u5148\u8fdb\u7684\u5f00\u6e90\u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u751a\u81f3\u5728\u57df\u5916\u8bbe\u7f6e\u4e2d\u63a5\u8fd1GPT-4o\u7b49\u4e13\u6709\u7cfb\u7edf\u7684\u6027\u80fd\u3002", "conclusion": "ChartReasoner\u80fd\u591f\u5c3d\u53ef\u80fd\u4fdd\u7559\u56fe\u8868\u7684\u539f\u59cb\u7ec6\u8282\uff0c\u5e76\u5728\u6027\u80fd\u4e0a\u4e0e\u6700\u5148\u8fdb\u7684\u5f00\u6e90\u6a21\u578b\u76f8\u5ab2\u7f8e\uff0c\u540c\u65f6\u4f7f\u7528\u66f4\u5c11\u7684\u53c2\u6570\uff0c\u5728\u57df\u5916\u8bbe\u7f6e\u4e2d\u63a5\u8fd1GPT-4o\u7b49\u4e13\u6709\u7cfb\u7edf\u7684\u6027\u80fd\u3002"}}
{"id": "2506.10139", "pdf": "https://arxiv.org/pdf/2506.10139", "abs": "https://arxiv.org/abs/2506.10139", "authors": ["Jiaxin Wen", "Zachary Ankner", "Arushi Somani", "Peter Hase", "Samuel Marks", "Jacob Goldman-Wetzler", "Linda Petrini", "Henry Sleight", "Collin Burns", "He He", "Shi Feng", "Ethan Perez", "Jan Leike"], "title": "Unsupervised Elicitation of Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "To steer pretrained language models for downstream tasks, today's\npost-training paradigm relies on humans to specify desired behaviors. However,\nfor models with superhuman capabilities, it is difficult or impossible to get\nhigh-quality human supervision. To address this challenge, we introduce a new\nunsupervised algorithm, Internal Coherence Maximization (ICM), to fine-tune\npretrained language models on their own generated labels, \\emph{without\nexternal supervision}. On GSM8k-verification, TruthfulQA, and Alpaca reward\nmodeling tasks, our method matches the performance of training on golden\nsupervision and outperforms training on crowdsourced human supervision. On\ntasks where LMs' capabilities are strongly superhuman, our method can elicit\nthose capabilities significantly better than training on human labels. Finally,\nwe show that our method can improve the training of frontier LMs: we use our\nmethod to train an unsupervised reward model and use reinforcement learning to\ntrain a Claude 3.5 Haiku-based assistant. Both the reward model and the\nassistant outperform their human-supervised counterparts.", "AI": {"tldr": "ICM\u662f\u4e00\u79cd\u65e0\u9700\u5916\u90e8\u76d1\u7763\u7684\u65e0\u76d1\u7763\u7b97\u6cd5\uff0c\u901a\u8fc7\u6a21\u578b\u81ea\u6211\u751f\u6210\u6807\u7b7e\u6765\u5fae\u8c03\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff0c\u5728\u591a\u9879\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u4e8e\u4eba\u7c7b\u76d1\u7763\uff0c\u5e76\u80fd\u66f4\u597d\u5730\u6fc0\u53d1\u8d85\u4eba\u80fd\u529b\uff0c\u63d0\u5347\u524d\u6cbf\u8bed\u8a00\u6a21\u578b\u7684\u8bad\u7ec3\u6548\u679c\u3002", "motivation": "\u5f53\u524d\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7684\u5fae\u8c03\u8303\u5f0f\u4f9d\u8d56\u4eba\u7c7b\u6307\u5b9a\u671f\u671b\u884c\u4e3a\uff0c\u4f46\u5bf9\u4e8e\u5177\u6709\u8d85\u4eba\u80fd\u529b\u7684\u6a21\u578b\uff0c\u83b7\u53d6\u9ad8\u8d28\u91cf\u7684\u4eba\u7c7b\u76d1\u7763\u53d8\u5f97\u56f0\u96be\u6216\u4e0d\u53ef\u80fd\u3002", "method": "\u5185\u90e8\u4e00\u81f4\u6027\u6700\u5927\u5316\uff08ICM\uff09\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u901a\u8fc7\u8bed\u8a00\u6a21\u578b\u81ea\u8eab\u751f\u6210\u7684\u6807\u7b7e\u8fdb\u884c\u5fae\u8c03\uff0c\u65e0\u9700\u5916\u90e8\u76d1\u7763\u3002", "result": "\u5728GSM8k-verification\u3001TruthfulQA\u548cAlpaca\u5956\u52b1\u5efa\u6a21\u4efb\u52a1\u4e0a\uff0cICM\u65b9\u6cd5\u8fbe\u5230\u4e86\u9ec4\u91d1\u76d1\u7763\u8bad\u7ec3\u7684\u6027\u80fd\uff0c\u5e76\u4f18\u4e8e\u4f17\u5305\u4eba\u7c7b\u76d1\u7763\u8bad\u7ec3\u3002\u8be5\u65b9\u6cd5\u80fd\u663e\u8457\u4f18\u4e8e\u4eba\u7c7b\u6807\u7b7e\uff0c\u66f4\u597d\u5730\u6fc0\u53d1\u8bed\u8a00\u6a21\u578b\u7684\u8d85\u4eba\u80fd\u529b\u3002\u6b64\u5916\uff0c\u901a\u8fc7ICM\u8bad\u7ec3\u7684\u65e0\u76d1\u7763\u5956\u52b1\u6a21\u578b\u548c\u57fa\u4e8eClaude 3.5 Haiku\u7684\u52a9\u624b\u90fd\u8d85\u8d8a\u4e86\u5176\u4eba\u7c7b\u76d1\u7763\u7684\u5bf9\u5e94\u7269\u3002", "conclusion": "ICM\u662f\u4e00\u79cd\u6709\u6548\u7684\u65e0\u76d1\u7763\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u6a21\u578b\u81ea\u6211\u751f\u6210\u7684\u6807\u7b7e\u6765\u5fae\u8c03\u8bed\u8a00\u6a21\u578b\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u5177\u6709\u8d85\u4eba\u80fd\u529b\u7684\u6a21\u578b\uff0c\u5e76\u80fd\u663e\u8457\u63d0\u5347\u524d\u6cbf\u8bed\u8a00\u6a21\u578b\u7684\u8bad\u7ec3\u6548\u679c\u3002"}}
