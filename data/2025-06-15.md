<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 2]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [A Survey of Automatic Evaluation Methods on Text, Visual and Speech Generations](https://arxiv.org/abs/2506.10019)
*Tian Lan,Yang-Hao Zhou,Zi-Ao Ma,Fanshu Sun,Rui-Qing Sun,Junyu Luo,Rong-Cheng Tu,Heyan Huang,Chen Xu,Zhijing Wu,Xian-Ling Mao*

Main category: cs.CL

TL;DR: 本文综述了文本、图像和音频生成内容的自动评估方法，并提出了一个统一的分类法，旨在解决当前缺乏系统性评估框架的问题。


<details>
  <summary>Details</summary>
Motivation: 当前研究缺乏一个系统性的框架，无法全面组织文本、视觉和音频模态的自动评估方法。

Method: 本文对文本、图像和音频三种模态的生成内容的自动评估方法进行了全面综述，并提出了一个统一的分类法，识别了表征现有评估方法的五种基本范式。

Result: 所提出的框架首先应用于文本生成评估方法，然后扩展到图像和音频生成，证明了其广泛适用性。

Conclusion: 讨论了跨模态评估方法未来研究的有前景方向。

Abstract: Recent advances in deep learning have significantly enhanced generative AI
capabilities across text, images, and audio. However, automatically evaluating
the quality of these generated outputs presents ongoing challenges. Although
numerous automatic evaluation methods exist, current research lacks a
systematic framework that comprehensively organizes these methods across text,
visual, and audio modalities. To address this issue, we present a comprehensive
review and a unified taxonomy of automatic evaluation methods for generated
content across all three modalities; We identify five fundamental paradigms
that characterize existing evaluation approaches across these domains. Our
analysis begins by examining evaluation methods for text generation, where
techniques are most mature. We then extend this framework to image and audio
generation, demonstrating its broad applicability. Finally, we discuss
promising directions for future research in cross-modal evaluation
methodologies.

</details>


### [2] [TaskCraft: Automated Generation of Agentic Tasks](https://arxiv.org/abs/2506.10055)
*Dingfeng Shi,Jingyi Cao,Qianben Chen,Weichen Sun,Weizhen Li,Hongxuan Lu,Fangchen Dong,Tianrui Qin,King Zhu,Minghao Yang,Jian Yang,Ge Zhang,Jiaheng Liu,Changwang Zhang,Jun Wang,Yuchen Eleanor Jiang,Wangchunshu Zhou*

Main category: cs.CL

TL;DR: 该论文介绍了TaskCraft，一个自动化生成多工具、可验证的代理任务的工作流，旨在解决现有代理任务数据和基准的局限性，并提供了一个包含约36,000个任务的大规模合成数据集。


<details>
  <summary>Details</summary>
Motivation: 代理任务在自然语言处理和人工智能领域日益重要，但现有指令数据缺乏工具交互，且当前代理基准依赖于昂贵的人工标注，限制了其可扩展性。

Method: 本文引入了TaskCraft，一个自动化的工作流，用于生成难度可扩展、多工具且可验证的代理任务，并附带执行轨迹。TaskCraft通过基于深度和宽度的扩展来扩展原子任务，从而创建结构和层次复杂的挑战。

Result: 实证结果表明，这些任务改进了生成工作流中的提示优化，并增强了代理基础模型的监督微调。

Conclusion: 本文提出了一个包含约36,000个不同难度任务的大规模合成数据集，以支持未来对代理调优和评估的研究。

Abstract: Agentic tasks, which require multi-step problem solving with autonomy, tool
use, and adaptive reasoning, are becoming increasingly central to the
advancement of NLP and AI. However, existing instruction data lacks tool
interaction, and current agentic benchmarks rely on costly human annotation,
limiting their scalability. We introduce \textsc{TaskCraft}, an automated
workflow for generating difficulty-scalable, multi-tool, and verifiable agentic
tasks with execution trajectories. TaskCraft expands atomic tasks using
depth-based and width-based extensions to create structurally and
hierarchically complex challenges. Empirical results show that these tasks
improve prompt optimization in the generation workflow and enhance supervised
fine-tuning of agentic foundation models. We present a large-scale synthetic
dataset of approximately 36,000 tasks with varying difficulty to support future
research on agent tuning and evaluation.

</details>
